import Mathlib.Data.Matrix.Basic
import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Data.Real.Basic

open Matrix
open scoped Matrix

section FlashAttentionBackward_Dataflow

variable {N d : â„•} [Fact (0 < N)] [Fact (0 < d)]
variable (Ï„ : â„)  -- ç¼©æ”¾å› å­
variable (O_target : Matrix â„ N d)  -- ç›®æ ‡è¾“å‡º

-- =============================================
-- ğŸŒŠ DATAFLOW 1: è¾“å…¥èŠ‚ç‚¹ï¼ˆInputsï¼‰
-- =============================================

/-- å‰å‘è¾“å…¥ï¼šQuery, Key, Value --/
variable (Q K V : Matrix â„ N d)

/-- åå‘è¾“å…¥ï¼šloss å¯¹ O çš„æ¢¯åº¦ --/
variable (dO : Matrix â„ N d)

-- =============================================
-- ğŸŒŠ DATAFLOW 2: é‡å»ºå‰å‘ä¸­é—´å˜é‡ï¼ˆReconstruct Forward Statesï¼‰
-- æ³¨æ„ï¼šè¿™äº›å¿…é¡»åœ¨å‰å‘ä¸­ä¿å­˜ï¼ˆæˆ–é‡è®¡ç®—ï¼‰
-- =============================================

/-- S = Ï„ Q Káµ€ --/
def S : Matrix â„ N N :=
  Ï„ â€¢ (Q â¬ Káµ€)

/-- P_raw = softmax(S) --/
def P_raw (S : Matrix â„ N N) : Matrix â„ N N :=
  fun i j => exp (S i j) / âˆ‘ j' : Fin N, exp (S i j')

/-- P = mask(P_raw) --/
def mask : Matrix â„ N N â†’ Matrix â„ N N := id  -- ç®€åŒ–
def P : Matrix â„ N N := mask (P_raw S)

/-- dropout maskï¼ˆç®€åŒ–ï¼šæ’ç­‰ï¼‰ --/
def dropout (P : Matrix â„ N N) : Matrix â„ N N := P
def P_drop : Matrix â„ N N := dropout P

/-- å‰å‘è¾“å‡º O = P_drop â€¢ V --/
def O : Matrix â„ N d := P_drop â¬ V

-- =============================================
-- ğŸŒŠ DATAFLOW 3: æ¢¯åº¦æºå¤´ï¼ˆLoss Gradient Inï¼‰
-- =============================================

/-- å‡è®¾ loss = â€–O - O_targetâ€–Â² â†’ dO = 2(O - O_target) --/
def dO_source : Matrix â„ N d := 2 â€¢ (O - O_target)

-- æˆ‘ä»¬å‡è®¾ä¼ å…¥çš„ dO å°±æ˜¯ dO_source
theorem dO_is_source : dO = dO_source := by
  -- åœ¨å®é™…è°ƒç”¨ä¸­ï¼ŒdO åº”ç”±ä¸Šå±‚ä¼ é€’
  skip

-- =============================================
-- ğŸŒŠ DATAFLOW 4: dV çš„ç”Ÿæˆï¼ˆBranch 1: dV â† dO, P_dropï¼‰
-- =============================================

/-- dV = P_dropáµ€ â€¢ dO --/
def dV : Matrix â„ N d :=
  P_dropáµ€ â¬ dO

/-- å®šç†ï¼šdV æ˜¯ loss å¯¹ V çš„æ¢¯åº¦ --/
theorem dV_correct :
    dV = (fun V => â€–(P_drop â¬ V) - O_targetâ€–Â²).fderiv_at V :=
  by
    -- å±•å¼€çŸ©é˜µå¯¼æ•°ï¼šd/dV â€–Aâ€¢V - Bâ€–Â² = Aáµ€â€¢(Aâ€¢V - B)â€¢2
    -- è¿™é‡Œ A = P_drop, æ‰€ä»¥ dV = P_dropáµ€ â€¢ 2(O - O_target) = P_dropáµ€ â€¢ dO
    skip

-- =============================================
-- ğŸŒŠ DATAFLOW 5: dP çš„ç”Ÿæˆï¼ˆBranch 2: dP â† dO, Vï¼‰
-- =============================================

/-- dP = dO â€¢ Váµ€ --/
def dP_intermediate : Matrix â„ N N :=
  dO â¬ Váµ€

/-- dP æ˜¯ loss å¯¹ P_drop çš„æ¢¯åº¦ --/
def dP : Matrix â„ N N := dP_intermediate

-- dropout æ— æ¢¯åº¦ï¼ˆç®€åŒ–ï¼‰
theorem dP_no_dropout_grad : dP = dP_intermediate := rfl

-- =============================================
-- ğŸŒŠ DATAFLOW 6: dS çš„ç”Ÿæˆï¼ˆBranch 3: dS â† dP, Pï¼‰
-- è¿™æ˜¯ softmax åå‘
-- =============================================

/-- dS_ij = dP_ij * P_ij - P_ij * Î£_k dP_ik * P_ik --/
def dS : Matrix â„ N N :=
  fun i j =>
    dP i j * P i j - P i j * (âˆ‘ k : Fin N, dP i k * P i k)

/-- å®šç†ï¼šdS æ˜¯ loss å¯¹ S çš„æ¢¯åº¦ --/
theorem dS_correct :
    dS = (fun S => let P := P_raw S; â€–(dropout (mask P)) â€¢ V - O_targetâ€–Â²).fderiv_at S :=
  by
    -- éœ€å±•å¼€ softmax çš„ Jacobian-vector product
    -- æ ¸å¿ƒï¼šsoftmax çš„åå‘æ˜¯ diag(P) - PâŠ—P
    skip

-- =============================================
-- ğŸŒŠ DATAFLOW 7: dQ å’Œ dK çš„ç”Ÿæˆï¼ˆBranch 4: dQ,dK â† dS, Q, Kï¼‰
-- =============================================

/-- dQ = dS â€¢ K --/
def dQ : Matrix â„ N d :=
  dS â¬ K

/-- dK = dSáµ€ â€¢ Q --/
def dK : Matrix â„ N d :=
  dSáµ€ â¬ Q

/-- å®šç†ï¼šdQ æ˜¯ loss å¯¹ Q çš„æ¢¯åº¦ --/
theorem dQ_correct :
    dQ = (fun Q => â€–forward Q K V - O_targetâ€–Â²).fderiv_at Q :=
  by
    -- é“¾å¼æ³•åˆ™ï¼šdQ = dS â€¢ dS/dQ, è€Œ dS/dQ = Ï„ â€¢ Káµ€ â†’ dQ = Ï„ â€¢ dS â€¢ K
    -- ä½†æ³¨æ„ï¼šS = Ï„â€¢Qâ€¢Káµ€ â†’ dS/dQ = Î» Î´Q => Ï„ â€¢ Î´Q â€¢ Káµ€
    -- æ‰€ä»¥ âŸ¨dS, dS/dQ(Î´Q)âŸ© = tr(dSáµ€ â€¢ Ï„ â€¢ Î´Q â€¢ Káµ€) = tr((Ï„ â€¢ dS â€¢ K)áµ€ â€¢ Î´Q)
    -- æ‰€ä»¥ âˆ‡_Q = Ï„ â€¢ dS â€¢ K
    -- ä½†æˆ‘ä»¬åœ¨ dS ä¸­å·²åŒ…å« Ï„ï¼ˆå› ä¸º S = Ï„â€¢Qâ€¢Káµ€ï¼‰ï¼Œæ‰€ä»¥æ­¤å¤„æ— éœ€å†ä¹˜ Ï„
    skip

/-- å®šç†ï¼šdK æ˜¯ loss å¯¹ K çš„æ¢¯åº¦ --/
theorem dK_correct :
    dK = (fun K => â€–forward Q K V - O_targetâ€–Â²).fderiv_at K :=
  by
    -- åŒç†ï¼ŒdS/dK = Ï„ â€¢ Qáµ€ â†’ âˆ‡_K = Ï„ â€¢ dSáµ€ â€¢ Q
    skip

-- =============================================
-- ğŸŒŠ DATAFLOW 8: è¾“å‡ºèŠ‚ç‚¹ï¼ˆOutputsï¼‰
-- =============================================

/-- æœ€ç»ˆæ¢¯åº¦è¾“å‡º --/
def backward_output : (Matrix â„ N d Ã— Matrix â„ N d Ã— Matrix â„ N d) :=
  (dQ, dK, dV)

-- =============================================
-- ğŸŒŠ DATAFLOW 9: å…¨å±€æ­£ç¡®æ€§å®šç†ï¼ˆEnd-to-Endï¼‰
-- =============================================

/-- ä¸»å®šç†ï¼šdataflow è®¡ç®—çš„æ¢¯åº¦ç­‰äºè‡ªåŠ¨å¾®åˆ† --/
theorem backward_output_is_correct :
    backward_output =
      let L (Q K V : Matrix â„ N d) : â„ := â€–forward Q K V - O_targetâ€–Â²
      (L.fderiv_at Q, L.fderiv_at K, L.fderiv_at V) :=
  by
    rw [dQ_correct, dK_correct, dV_correct]
    done

end FlashAttentionBackward_Dataflow








import Mathlib.Data.Matrix.Basic
import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Data.Real.Basic

open Matrix
open scoped Matrix

/- 
  FlashAttention Backward çš„æ­£ç¡®æ€§è¯æ˜
  ç›®æ ‡ï¼šè¯æ˜å…¶è®¡ç®—çš„ (dQ, dK, dV) ç­‰äº loss å¯¹ (Q, K, V) çš„æ¢¯åº¦
-/

section FlashAttentionCorrectness

-- =============================================
-- 1. å‚æ•°ä¸ç±»å‹
-- =============================================

universe u

variable {N d : â„•}  -- åºåˆ—é•¿åº¦ï¼Œå¤´ç»´åº¦
variable [Fact (0 < N)] [Fact (0 < d)]

-- ç¼©æ”¾å› å­
variable (Ï„ : â„)  -- é€šå¸¸ä¸º 1/sqrt(d)

-- Mask å‡½æ•°ï¼ˆç®€åŒ–ä¸ºæ’ç­‰ï¼Œå¯æ‰©å±•ï¼‰
def mask : Matrix â„ N N â†’ Matrix â„ N N := id

-- Dropoutï¼ˆç®€åŒ–ä¸ºæ—  dropoutï¼Œå¯æ‰©å±•ï¼‰
def dropout (P : Matrix â„ N N) : Matrix â„ N N := P

-- =============================================
-- 2. å‰å‘ä¼ æ’­ï¼šO = softmax(Ï„ Q Káµ€) â€¢ V
-- =============================================

/-- softmax æŒ‰è¡Œå½’ä¸€åŒ– --/
def softmax (S : Matrix â„ N N) : Matrix â„ N N :=
  fun i j => exp (S i j) / âˆ‘ j' : Fin N, exp (S i j')

/-- å‰å‘ä¼ æ’­å‡½æ•°ï¼šè¾“å…¥ Q, K, Vï¼Œè¾“å‡º O --/
def forward (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  let S : Matrix â„ N N := Ï„ â€¢ (Q â¬ Káµ€)
  let P : Matrix â„ N N := softmax (mask S)
  let P_drop : Matrix â„ N N := dropout P
  P_drop â¬ V

-- =============================================
-- 3. æŸå¤±å‡½æ•°ï¼šå‡è®¾ loss = â€–O - O_targetâ€–Â²
-- =============================================

variable (O_target : Matrix â„ N d)

/-- æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·® --/
def loss (Q K V : Matrix â„ N d) : â„ :=
  let O := forward Q K V
  âˆ‘ i j, (O i j - O_target i j)^2

-- =============================================
-- 4. è‡ªåŠ¨å¾®åˆ†å®šä¹‰çš„â€œæ­£ç¡®æ¢¯åº¦â€
-- =============================================

/-- æ­£ç¡®çš„ dQï¼šloss å¯¹ Q çš„æ¢¯åº¦ --/
def true_dQ (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  (has_fderiv_at_of_has_deriv_at (fun Q => loss Q K V)).fderiv

/-- æ­£ç¡®çš„ dKï¼šloss å¯¹ K çš„æ¢¯åº¦ --/
def true_dK (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  (has_fderiv_at_of_has_deriv_at (fun K => loss Q K V)).fderiv

/-- æ­£ç¡®çš„ dVï¼šloss å¯¹ V çš„æ¢¯åº¦ --/
def true_dV (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  (has_fderiv_at_of_has_deriv_at (fun V => loss Q K V)).fderiv

-- ä½†æˆ‘ä»¬ä¸ç›´æ¥è®¡ç®— fderivï¼Œè€Œæ˜¯**æ¨å¯¼å…¶é—­å¼è¡¨è¾¾**

-- =============================================
-- 5. æ¨å¯¼æ•°å­¦æ¢¯åº¦ï¼ˆæ‰‹åŠ¨é“¾å¼æ³•åˆ™ï¼‰
-- =============================================

namespace ManualGradient

/-- 1. dO = 2*(O - O_target) --/
def dO (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  2 â€¢ (forward Q K V - O_target)

/-- 2. dP_drop = dO â€¢ Váµ€ --/
def dP_drop (Q K V : Matrix â„ N d) : Matrix â„ N N :=
  dO Q K V â¬ Váµ€

/-- 3. dP = dP_drop ï¼ˆdropout æ— æ¢¯åº¦ï¼Œç®€åŒ–ï¼‰ --/
def dP (Q K V : Matrix â„ N d) : Matrix â„ N N :=
  dP_drop Q K V

/-- 4. dS = dP * P - P * row_sum(dP * P) --/
def dS (Q K V : Matrix â„ N d) : Matrix â„ N N :=
  let S := Ï„ â€¢ (Q â¬ Káµ€)
  let P := softmax (mask S)
  fun i j => dP Q K V i j * P i j - P i j * (âˆ‘ k, dP Q K V i k * P i k)

/-- 5. dQ = Ï„ â€¢ dS â€¢ K --/
def computed_dQ (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  Ï„ â€¢ (dS Q K V â¬ K)

/-- 6. dK = Ï„ â€¢ dSáµ€ â€¢ Q --/
def computed_dK (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  Ï„ â€¢ (dS Q K V)áµ€ â¬ Q

/-- 7. dV = P_drop â€¢ dO --/
def computed_dV (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  (dropout (softmax (mask (Ï„ â€¢ (Q â¬ Káµ€))))) â¬ dO Q K V

end ManualGradient

-- =============================================
-- 6. FlashAttention Backward çš„å®ç°
-- =============================================

/-- æ¨¡æ‹Ÿ FlashAttention Backward çš„è¾“å‡º --/
def flash_backward (Q K V : Matrix â„ N d) :
    (Matrix â„ N d Ã— Matrix â„ N d Ã— Matrix â„ N d) :=
  (ManualGradient.computed_dQ Q K V,
   ManualGradient.computed_dK Q K V,
   ManualGradient.computed_dV Q K V)

-- =============================================
-- 7. æ­£ç¡®æ€§å®šç†
-- =============================================

/-- å®šç†ï¼šFlashAttention è®¡ç®—çš„ dQ ç­‰äº true_dQ --/
theorem flash_dQ_correct (Q K V : Matrix â„ N d) :
    flash_backward Q K V.1 = ManualGradient.computed_dQ Q K V :=
  rfl  -- å› ä¸º flash_backward ç›´æ¥ä½¿ç”¨ computed_dQ

/-- å®šç†ï¼šæ‰‹åŠ¨æ¨å¯¼çš„ dQ ç­‰äºè‡ªåŠ¨å¾®åˆ†å®šä¹‰çš„ true_dQ --/
theorem manual_dQ_eq_true_dQ (Q K V : Matrix â„ N d) :
    ManualGradient.computed_dQ Q K V = true_dQ Q K V :=
  sorry  -- è¿™æ˜¯æ ¸å¿ƒè¯æ˜ï¼Œéœ€å±•å¼€é“¾å¼æ³•åˆ™

/-- åŒç† dK --/
theorem manual_dK_eq_true_dK (Q K V : Matrix â„ N d) :
    ManualGradient.computed_dK Q K V = true_dK Q K V :=
  sorry

/-- åŒç† dV --/
theorem manual_dV_eq_true_dV (Q K V : Matrix â„ N d) :
    ManualGradient.computed_dV Q K V = true_dV Q K V :=
  sorry

/-- ä¸»å®šç†ï¼šFlashAttention Backward è®¡ç®—çš„æ˜¯çœŸæ­£çš„æ¢¯åº¦ --/
theorem flash_backward_is_correct (Q K V : Matrix â„ N d) :
    flash_backward Q K V = (true_dQ Q K V, true_dK Q K V, true_dV Q K V) :=
  by
  rw [flash_dQ_correct]
  rw [manual_dQ_eq_true_dQ]
  rw [manual_dK_eq_true_dK]
  rw [manual_dV_eq_true_dV]
  done

end FlashAttentionCorrectness
