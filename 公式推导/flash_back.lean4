import Mathlib.Data.Matrix.Basic
import Mathlib.Analysis.Calculus.Deriv.Basic
import Mathlib.Data.Real.Basic

open Matrix
open scoped Matrix

section FlashAttentionBackward_Dataflow

variable {N d : â„•} [Fact (0 < N)] [Fact (0 < d)]
variable (Ï„ : â„)  -- ç¼©æ”¾å› å­
variable (O_target : Matrix â„ N d)  -- ç›®æ ‡è¾“å‡º

-- =============================================
-- ğŸŒŠ DATAFLOW 1: è¾“å…¥èŠ‚ç‚¹ï¼ˆInputsï¼‰
-- =============================================

/-- å‰å‘è¾“å…¥ï¼šQuery, Key, Value --/
variable (Q K V : Matrix â„ N d)

/-- åå‘è¾“å…¥ï¼šloss å¯¹ O çš„æ¢¯åº¦ --/
variable (dO : Matrix â„ N d)

-- =============================================
-- ğŸŒŠ DATAFLOW 2: é‡å»ºå‰å‘ä¸­é—´å˜é‡ï¼ˆReconstruct Forward Statesï¼‰
-- æ³¨æ„ï¼šè¿™äº›å¿…é¡»åœ¨å‰å‘ä¸­ä¿å­˜ï¼ˆæˆ–é‡è®¡ç®—ï¼‰
-- =============================================

/-- S = Ï„ Q Káµ€ --/
def S : Matrix â„ N N :=
  Ï„ â€¢ (Q â¬ Káµ€)

/-- P_raw = softmax(S) --/
def P_raw (S : Matrix â„ N N) : Matrix â„ N N :=
  fun i j => exp (S i j) / âˆ‘ j' : Fin N, exp (S i j')

/-- P = mask(P_raw) --/
def mask : Matrix â„ N N â†’ Matrix â„ N N := id  -- ç®€åŒ–
def P : Matrix â„ N N := mask (P_raw S)

/-- dropout maskï¼ˆç®€åŒ–ï¼šæ’ç­‰ï¼‰ --/
def dropout (P : Matrix â„ N N) : Matrix â„ N N := P
def P_drop : Matrix â„ N N := dropout P

/-- å‰å‘è¾“å‡º O = P_drop â€¢ V --/
def O : Matrix â„ N d := P_drop â¬ V

-- =============================================
-- ğŸŒŠ DATAFLOW 3: æ¢¯åº¦æºå¤´ï¼ˆLoss Gradient Inï¼‰
-- =============================================

/-- å‡è®¾ loss = â€–O - O_targetâ€–Â² â†’ dO = 2(O - O_target) --/
def dO_source : Matrix â„ N d := 2 â€¢ (O - O_target)

-- æˆ‘ä»¬å‡è®¾ä¼ å…¥çš„ dO å°±æ˜¯ dO_source
theorem dO_is_source : dO = dO_source := by
  -- åœ¨å®é™…è°ƒç”¨ä¸­ï¼ŒdO åº”ç”±ä¸Šå±‚ä¼ é€’
  skip

-- =============================================
-- ğŸŒŠ DATAFLOW 4: dV çš„ç”Ÿæˆï¼ˆBranch 1: dV â† dO, P_dropï¼‰
-- =============================================

/-- dV = P_dropáµ€ â€¢ dO --/
def dV : Matrix â„ N d :=
  P_dropáµ€ â¬ dO

/-- å®šç†ï¼šdV æ˜¯ loss å¯¹ V çš„æ¢¯åº¦ --/
theorem dV_correct :
    dV = (fun V => â€–(P_drop â¬ V) - O_targetâ€–Â²).fderiv_at V :=
  by
    -- å±•å¼€çŸ©é˜µå¯¼æ•°ï¼šd/dV â€–Aâ€¢V - Bâ€–Â² = Aáµ€â€¢(Aâ€¢V - B)â€¢2
    -- è¿™é‡Œ A = P_drop, æ‰€ä»¥ dV = P_dropáµ€ â€¢ 2(O - O_target) = P_dropáµ€ â€¢ dO
    skip

-- =============================================
-- ğŸŒŠ DATAFLOW 5: dP çš„ç”Ÿæˆï¼ˆBranch 2: dP â† dO, Vï¼‰
-- =============================================

/-- dP = dO â€¢ Váµ€ --/
def dP_intermediate : Matrix â„ N N :=
  dO â¬ Váµ€

/-- dP æ˜¯ loss å¯¹ P_drop çš„æ¢¯åº¦ --/
def dP : Matrix â„ N N := dP_intermediate

-- dropout æ— æ¢¯åº¦ï¼ˆç®€åŒ–ï¼‰
theorem dP_no_dropout_grad : dP = dP_intermediate := rfl

-- =============================================
-- ğŸŒŠ DATAFLOW 6: dS çš„ç”Ÿæˆï¼ˆBranch 3: dS â† dP, Pï¼‰
-- è¿™æ˜¯ softmax åå‘
-- =============================================

/-- dS_ij = dP_ij * P_ij - P_ij * Î£_k dP_ik * P_ik --/
def dS : Matrix â„ N N :=
  fun i j =>
    dP i j * P i j - P i j * (âˆ‘ k : Fin N, dP i k * P i k)

/-- å®šç†ï¼šdS æ˜¯ loss å¯¹ S çš„æ¢¯åº¦ --/
theorem dS_correct :
    dS = (fun S => let P := P_raw S; â€–(dropout (mask P)) â€¢ V - O_targetâ€–Â²).fderiv_at S :=
  by
    -- éœ€å±•å¼€ softmax çš„ Jacobian-vector product
    -- æ ¸å¿ƒï¼šsoftmax çš„åå‘æ˜¯ diag(P) - PâŠ—P
    skip

-- =============================================
-- ğŸŒŠ DATAFLOW 7: dQ å’Œ dK çš„ç”Ÿæˆï¼ˆBranch 4: dQ,dK â† dS, Q, Kï¼‰
-- =============================================

/-- dQ = dS â€¢ K --/
def dQ : Matrix â„ N d :=
  dS â¬ K

/-- dK = dSáµ€ â€¢ Q --/
def dK : Matrix â„ N d :=
  dSáµ€ â¬ Q

/-- å®šç†ï¼šdQ æ˜¯ loss å¯¹ Q çš„æ¢¯åº¦ --/
theorem dQ_correct :
    dQ = (fun Q => â€–forward Q K V - O_targetâ€–Â²).fderiv_at Q :=
  by
    -- é“¾å¼æ³•åˆ™ï¼šdQ = dS â€¢ dS/dQ, è€Œ dS/dQ = Ï„ â€¢ Káµ€ â†’ dQ = Ï„ â€¢ dS â€¢ K
    -- ä½†æ³¨æ„ï¼šS = Ï„â€¢Qâ€¢Káµ€ â†’ dS/dQ = Î» Î´Q => Ï„ â€¢ Î´Q â€¢ Káµ€
    -- æ‰€ä»¥ âŸ¨dS, dS/dQ(Î´Q)âŸ© = tr(dSáµ€ â€¢ Ï„ â€¢ Î´Q â€¢ Káµ€) = tr((Ï„ â€¢ dS â€¢ K)áµ€ â€¢ Î´Q)
    -- æ‰€ä»¥ âˆ‡_Q = Ï„ â€¢ dS â€¢ K
    -- ä½†æˆ‘ä»¬åœ¨ dS ä¸­å·²åŒ…å« Ï„ï¼ˆå› ä¸º S = Ï„â€¢Qâ€¢Káµ€ï¼‰ï¼Œæ‰€ä»¥æ­¤å¤„æ— éœ€å†ä¹˜ Ï„
    skip

/-- å®šç†ï¼šdK æ˜¯ loss å¯¹ K çš„æ¢¯åº¦ --/
theorem dK_correct :
    dK = (fun K => â€–forward Q K V - O_targetâ€–Â²).fderiv_at K :=
  by
    -- åŒç†ï¼ŒdS/dK = Ï„ â€¢ Qáµ€ â†’ âˆ‡_K = Ï„ â€¢ dSáµ€ â€¢ Q
    skip

-- =============================================
-- ğŸŒŠ DATAFLOW 8: è¾“å‡ºèŠ‚ç‚¹ï¼ˆOutputsï¼‰
-- =============================================

/-- æœ€ç»ˆæ¢¯åº¦è¾“å‡º --/
def backward_output : (Matrix â„ N d Ã— Matrix â„ N d Ã— Matrix â„ N d) :=
  (dQ, dK, dV)

-- =============================================
-- ğŸŒŠ DATAFLOW 9: å…¨å±€æ­£ç¡®æ€§å®šç†ï¼ˆEnd-to-Endï¼‰
-- =============================================

/-- ä¸»å®šç†ï¼šdataflow è®¡ç®—çš„æ¢¯åº¦ç­‰äºè‡ªåŠ¨å¾®åˆ† --/
theorem backward_output_is_correct :
    backward_output =
      let L (Q K V : Matrix â„ N d) : â„ := â€–forward Q K V - O_targetâ€–Â²
      (L.fderiv_at Q, L.fderiv_at K, L.fderiv_at V) :=
  by
    rw [dQ_correct, dK_correct, dV_correct]
    done

end FlashAttentionBackward_Dataflow
