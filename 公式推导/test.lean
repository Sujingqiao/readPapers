import Mathlib.Data.Fintype.Basic
import Mathlib.Data.Finset
import Mathlib.Algebra.BigOperators.Basic
import Mathlib.LinearAlgebra.Matrix
import Mathlib.Analysis.SpecialFunctions.Trigonometric

-- 1. å®šä¹‰ç»´åº¦ï¼ˆç”¨æœ‰é™ç±»å‹è¡¨ç¤ºç´¢å¼•ï¼‰
abbrev TokenCount := Fin 512    -- åºåˆ—é•¿åº¦ï¼Œå¦‚ 512
abbrev EmbedDim := Fin 512      -- åµŒå…¥ç»´åº¦
abbrev HeadCount := Fin 8       -- æ³¨æ„åŠ›å¤´æ•°
abbrev HiddenDim := Fin 2048    -- FFN éšè—å±‚ç»´åº¦

-- 2. å‘é‡ä¸çŸ©é˜µï¼ˆä½¿ç”¨ Mathlib çš„çŸ©é˜µç±»å‹ï¼‰
abbrev Vec (n : Type) [Fintype n] := n â†’ â„
abbrev Mat (m n : Type) [Fintype m] [Fintype n] := m â†’ n â†’ â„

-- 3. åµŒå…¥å‘é‡ç±»å‹
def Embedding := EmbedDim â†’ â„
def TokenEmbedding := TokenCount â†’ Embedding  -- è¯ç¬¦åµŒå…¥è¡¨

-- 4. ä½ç½®ç¼–ç ï¼ˆç®€åŒ–ç‰ˆï¼šæ­£å¼¦ç¼–ç ï¼‰
def positionalEncoding (pos : TokenCount) (i : EmbedDim) : â„ :=
  if i.val % 2 = 0 then
    Real.sin (pos.val / 10000^(i.val / 512.0))
  else
    Real.cos (pos.val / 10000^((i.val-1) / 512.0))

def PositionalEncoding := TokenCount â†’ Embedding
def posEnc : PositionalEncoding := positionalEncoding

-- 5. çº¿æ€§å˜æ¢ï¼šçŸ©é˜µä¹˜æ³•
def Linear (inDim outDim : Type) [Fintype inDim] [Fintype outDim] :=
  { W : Mat outDim inDim, b : Vec outDim }

def applyLinear (L : Linear inDim outDim) (x : Vec inDim) : Vec outDim :=
  fun j => âˆ‘ i, L.W j i * x i + L.b j

-- 6. Layer Normalization
def LayerNorm (dim : Type) [Fintype dim] :=
  { gamma : Vec dim, beta : Vec dim, eps : â„ }

def mean (x : Vec dim) : â„ := (âˆ‘ i, x i) / Fintype.card dim

def variance (x : Vec dim) (Î¼ : â„) : â„ := (âˆ‘ i, (x i - Î¼)^2) / Fintype.card dim

def applyLayerNorm (ln : LayerNorm dim) (x : Vec dim) : Vec dim := 
  let Î¼ := mean x
  let Ïƒ := Real.sqrt (variance x Î¼ + ln.eps)
  fun i => ln.gamma i * (x i - Î¼) / Ïƒ + ln.beta i

-- 7. æ³¨æ„åŠ›å¤´çš„è¾“å…¥è¾“å‡º
def Query := EmbedDim â†’ â„
def Key   := EmbedDim â†’ â„
def Value := EmbedDim â†’ â„

-- 8. å•å¤´æ³¨æ„åŠ›
def SingleHeadAttention :=
  { Wq : Linear EmbedDim EmbedDim
  , Wk : Linear EmbedDim EmbedDim
  , Wv : Linear EmbedDim EmbedDim
  , Wo : Linear EmbedDim EmbedDim  -- è¾“å‡ºæŠ•å½±
  }

-- 9. softmax å‡½æ•°ï¼ˆç®€åŒ–ï¼šå®šä¹‰åœ¨å‘é‡ä¸Šï¼‰
def softmax (z : Vec TokenCount) : Vec TokenCount :=
  let Z := âˆ‘ j, Real.exp (z j)
  fun i => Real.exp (z i) / Z

-- 10. å®ç°å•å¤´æ³¨æ„åŠ›è®¡ç®—
def computeAttention (head : SingleHeadAttention) 
                      (x : TokenCount â†’ Embedding) 
                      : TokenCount â†’ Embedding := 
  -- 1. è®¡ç®— Q, K, V
  let Q := fun (t : TokenCount) => applyLinear head.Wq (x t)
  let K := fun (t : TokenCount) => applyLinear head.Wk (x t)
  let V := fun (t : TokenCount) => applyLinear head.Wv (x t)
  
  -- 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
  let scores : TokenCount â†’ TokenCount â†’ â„ := 
    fun t1 t2 => (âˆ‘ d : EmbedDim, Q t1 d * K t2 d) / Real.sqrt (512.0)
  
  -- 3. åº”ç”¨ softmaxï¼ˆå¯¹æ¯ä¸ªè¡Œï¼‰
  let attn_weights : TokenCount â†’ TokenCount â†’ â„ :=
    fun t1 => softmax (fun t2 => scores t1 t2)
  
  -- 4. åŠ æƒæ±‚å’Œ V
  fun t_out => 
    fun d_out => 
      âˆ‘ t_in : TokenCount, attn_weights t_out t_in * V t_in d_out

-- 11. å¤šå¤´æ³¨æ„åŠ›ï¼šHeadCount ä¸ªå¤´ + è¾“å‡ºæŠ•å½±
def MultiHeadAttention :=
  { heads : HeadCount â†’ SingleHeadAttention
  , W_o : Linear EmbedDim EmbedDim  -- æ‹¼æ¥åçš„æŠ•å½±
  }

-- 12. å®ç°å¤šå¤´æ³¨æ„åŠ›
def computeMultiHead (mha : MultiHeadAttention) 
                      (x : TokenCount â†’ Embedding) 
                      : TokenCount â†’ Embedding := 
  -- æ¯ä¸ªå¤´è¾“å‡ºä¸€ä¸ª Embedding
  let head_outputs : HeadCount â†’ (TokenCount â†’ Embedding) :=
    fun h => computeAttention (mha.heads h) x
  
  -- ç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªå¤´è¾“å‡ºå…¨ç»´åº¦ï¼ˆå®é™…æ˜¯ d_kï¼Œè¿™é‡Œçœç•¥æ‹†åˆ†ï¼‰
  -- å®é™…ä¸­éœ€å®šä¹‰æŠ•å½±åˆ° d_kï¼Œå†æ‹¼æ¥ï¼Œå†æŠ•å½±å› d_model
  -- è¿™é‡Œç®€åŒ–ä¸ºï¼šç›´æ¥å¯¹æ¯ä¸ªå¤´è¾“å‡ºåŠ æƒå¹³å‡ï¼ˆæˆ–æ±‚å’Œï¼‰
  -- æ›´ç²¾ç¡®çš„æ¨¡å‹åº”å¼•å…¥ HeadDim := Fin 64 å¹¶å®šä¹‰æ‹¼æ¥æ“ä½œ
  
  -- ç®€åŒ–ç‰ˆæœ¬ï¼šæ±‚å’Œ + æŠ•å½±
  let summed : TokenCount â†’ Embedding :=
    fun t => fun d => âˆ‘ h : HeadCount, head_outputs h t d
  
  -- åº”ç”¨è¾“å‡ºæŠ•å½±
  fun t => applyLinear mha.W_o (summed t)

-- 13. å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰
def FFN :=
  { W1 : Linear EmbedDim HiddenDim
  , W2 : Linear HiddenDim EmbedDim
  , b1 : Vec HiddenDim
  , b2 : Vec EmbedDim
  }

def applyFFN (ffn : FFN) (x : Embedding) : Embedding :=
  let h := applyLinear ffn.W1 x + ffn.b1  -- çº¿æ€§ + åç½®
  let h_gelu := fun i => Real.gelu (h i)  -- GELU æ¿€æ´»ï¼ˆéœ€å¯¼å…¥æˆ–å®šä¹‰ï¼‰
  applyLinear ffn.W2 h_gelu + ffn.b2

-- 14. ç¼–ç å™¨å±‚
def EncoderLayer :=
  { mha : MultiHeadAttention
  , ffn : FFN
  , ln1 : LayerNorm EmbedDim
  , ln2 : LayerNorm EmbedDim
  }

-- 15. å®ç°ç¼–ç å™¨å±‚å‰å‘ä¼ æ’­
def forwardLayer (layer : EncoderLayer) 
                 (x : TokenCount â†’ Embedding) 
                 : TokenCount â†’ Embedding := 
  let x1 := fun t => applyLayerNorm layer.ln1 (x t + computeMultiHead layer.mha x t)
  let x2 := fun t => applyLayerNorm layer.ln2 (x1 t + applyFFN layer.ffn (x1 t))
  x2

-- 16. å‡è®¾ 6 å±‚ç¼–ç å™¨
abbrev LayerIdx := Fin 6

def TransformerEncoder :=
  LayerIdx â†’ EncoderLayer

def applyEncoder (encoder : TransformerEncoder) 
                 (x : TokenCount â†’ Embedding) 
                 : TokenCount â†’ Embedding := 
  let x_embed := fun t => x t  -- åˆå§‹åµŒå…¥ï¼ˆå®é™…ä¸­éœ€æŸ¥è¡¨ï¼‰
  let x_pos := fun t => fun d => x_embed t d + posEnc t d  -- åŠ ä½ç½®ç¼–ç 
  -- ç®€åŒ–ï¼šåªå®ç°ä¸€å±‚ï¼Œå¯é€’å½’å †å 
  forwardLayer (encoder 0) x_pos
  -- å¯æ‰©å±•ä¸ºé€’å½’ï¼šfoldl æˆ–é€’å½’å‡½æ•° over LayerIdx

theorem attention_weights_sum_to_one (head : SingleHeadAttention) 
                                     (x : TokenCount â†’ Embedding) 
                                     (t_out : TokenCount) :
    âˆ‘ t_in : TokenCount, (let scores := fun t1 t2 => ...; 
                          softmax (fun t2 => scores t_out t2)) t_in = 1 := by
  -- å±•å¼€ softmax å®šä¹‰ï¼Œåˆ©ç”¨ âˆ‘ exp / Z = Z/Z = 1
  simp [softmax]
  ring
  -- éœ€è¦è¯æ˜ Z â‰  0ï¼Œé€šå¸¸æˆç«‹

theorem residual_preserves_type :
    âˆ€ x : TokenCount â†’ Embedding,
       (fun t => x t + computeMultiHead mha x t) = 
       (some_other_expr t) â†’ 
       -- ç±»å‹è‡ªåŠ¨ä¿è¯ï¼šè¾“å‡ºä»æ˜¯ TokenCount â†’ Embedding
       True := by trivial





import Mathlib.Data.Fintype.Basic
import Mathlib.Data.Finset
import Mathlib.Algebra.BigOperators.Basic
import Mathlib.LinearAlgebra.Matrix
import Mathlib.Analysis.SpecialFunctions.Trigonometric

-- 1. å®šä¹‰ç»´åº¦ï¼ˆç”¨æœ‰é™ç±»å‹è¡¨ç¤ºç´¢å¼•ï¼‰
abbrev TokenCount := Fin 512    -- åºåˆ—é•¿åº¦ï¼Œå¦‚ 512
abbrev EmbedDim := Fin 512      -- åµŒå…¥ç»´åº¦
abbrev HeadCount := Fin 8       -- æ³¨æ„åŠ›å¤´æ•°
abbrev HiddenDim := Fin 2048    -- FFN éšè—å±‚ç»´åº¦

-- 2. å‘é‡ä¸çŸ©é˜µï¼ˆä½¿ç”¨ Mathlib çš„çŸ©é˜µç±»å‹ï¼‰
abbrev Vec (n : Type) [Fintype n] := n â†’ â„
abbrev Mat (m n : Type) [Fintype m] [Fintype n] := m â†’ n â†’ â„

-- 3. åµŒå…¥å‘é‡ç±»å‹
def Embedding := EmbedDim â†’ â„
def TokenEmbedding := TokenCount â†’ Embedding  -- è¯ç¬¦åµŒå…¥è¡¨

-- 4. ä½ç½®ç¼–ç ï¼ˆç®€åŒ–ç‰ˆï¼šæ­£å¼¦ç¼–ç ï¼‰
def positionalEncoding (pos : TokenCount) (i : EmbedDim) : â„ :=
  if i.val % 2 = 0 then
    Real.sin (pos.val / 10000^(i.val / 512.0))
  else
    Real.cos (pos.val / 10000^((i.val-1) / 512.0))

def PositionalEncoding := TokenCount â†’ Embedding
def posEnc : PositionalEncoding := positionalEncoding

-- 5. çº¿æ€§å˜æ¢ï¼šçŸ©é˜µä¹˜æ³•
def Linear (inDim outDim : Type) [Fintype inDim] [Fintype outDim] :=
  { W : Mat outDim inDim, b : Vec outDim }

def applyLinear (L : Linear inDim outDim) (x : Vec inDim) : Vec outDim :=
  fun j => âˆ‘ i, L.W j i * x i + L.b j

-- 6. Layer Normalization
def LayerNorm (dim : Type) [Fintype dim] :=
  { gamma : Vec dim, beta : Vec dim, eps : â„ }

def mean (x : Vec dim) : â„ := (âˆ‘ i, x i) / Fintype.card dim

def variance (x : Vec dim) (Î¼ : â„) : â„ := (âˆ‘ i, (x i - Î¼)^2) / Fintype.card dim

def applyLayerNorm (ln : LayerNorm dim) (x : Vec dim) : Vec dim := 
  let Î¼ := mean x
  let Ïƒ := Real.sqrt (variance x Î¼ + ln.eps)
  fun i => ln.gamma i * (x i - Î¼) / Ïƒ + ln.beta i

-- 7. æ³¨æ„åŠ›å¤´çš„è¾“å…¥è¾“å‡º
def Query := EmbedDim â†’ â„
def Key   := EmbedDim â†’ â„
def Value := EmbedDim â†’ â„

-- 8. å•å¤´æ³¨æ„åŠ›
def SingleHeadAttention :=
  { Wq : Linear EmbedDim EmbedDim
  , Wk : Linear EmbedDim EmbedDim
  , Wv : Linear EmbedDim EmbedDim
  , Wo : Linear EmbedDim EmbedDim  -- è¾“å‡ºæŠ•å½±
  }

-- 9. softmax å‡½æ•°ï¼ˆç®€åŒ–ï¼šå®šä¹‰åœ¨å‘é‡ä¸Šï¼‰
def softmax (z : Vec TokenCount) : Vec TokenCount :=
  let Z := âˆ‘ j, Real.exp (z j)
  fun i => Real.exp (z i) / Z

-- 10. å®ç°å•å¤´æ³¨æ„åŠ›è®¡ç®—
def computeAttention (head : SingleHeadAttention) 
                      (x : TokenCount â†’ Embedding) 
                      : TokenCount â†’ Embedding := 
  -- 1. è®¡ç®— Q, K, V
  let Q := fun (t : TokenCount) => applyLinear head.Wq (x t)
  let K := fun (t : TokenCount) => applyLinear head.Wk (x t)
  let V := fun (t : TokenCount) => applyLinear head.Wv (x t)
  
  -- 2. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
  let scores : TokenCount â†’ TokenCount â†’ â„ := 
    fun t1 t2 => (âˆ‘ d : EmbedDim, Q t1 d * K t2 d) / Real.sqrt (512.0)
  
  -- 3. åº”ç”¨ softmaxï¼ˆå¯¹æ¯ä¸ªè¡Œï¼‰
  let attn_weights : TokenCount â†’ TokenCount â†’ â„ :=
    fun t1 => softmax (fun t2 => scores t1 t2)
  
  -- 4. åŠ æƒæ±‚å’Œ V
  fun t_out => 
    fun d_out => 
      âˆ‘ t_in : TokenCount, attn_weights t_out t_in * V t_in d_out

-- 11. å¤šå¤´æ³¨æ„åŠ›ï¼šHeadCount ä¸ªå¤´ + è¾“å‡ºæŠ•å½±
def MultiHeadAttention :=
  { heads : HeadCount â†’ SingleHeadAttention
  , W_o : Linear EmbedDim EmbedDim  -- æ‹¼æ¥åçš„æŠ•å½±
  }

-- 12. å®ç°å¤šå¤´æ³¨æ„åŠ›
def computeMultiHead (mha : MultiHeadAttention) 
                      (x : TokenCount â†’ Embedding) 
                      : TokenCount â†’ Embedding := 
  -- æ¯ä¸ªå¤´è¾“å‡ºä¸€ä¸ª Embedding
  let head_outputs : HeadCount â†’ (TokenCount â†’ Embedding) :=
    fun h => computeAttention (mha.heads h) x
  
  -- ç®€åŒ–ï¼šå‡è®¾æ¯ä¸ªå¤´è¾“å‡ºå…¨ç»´åº¦ï¼ˆå®é™…æ˜¯ d_kï¼Œè¿™é‡Œçœç•¥æ‹†åˆ†ï¼‰
  -- å®é™…ä¸­éœ€å®šä¹‰æŠ•å½±åˆ° d_kï¼Œå†æ‹¼æ¥ï¼Œå†æŠ•å½±å› d_model
  -- è¿™é‡Œç®€åŒ–ä¸ºï¼šç›´æ¥å¯¹æ¯ä¸ªå¤´è¾“å‡ºåŠ æƒå¹³å‡ï¼ˆæˆ–æ±‚å’Œï¼‰
  -- æ›´ç²¾ç¡®çš„æ¨¡å‹åº”å¼•å…¥ HeadDim := Fin 64 å¹¶å®šä¹‰æ‹¼æ¥æ“ä½œ
  
  -- ç®€åŒ–ç‰ˆæœ¬ï¼šæ±‚å’Œ + æŠ•å½±
  let summed : TokenCount â†’ Embedding :=
    fun t => fun d => âˆ‘ h : HeadCount, head_outputs h t d
  
  -- åº”ç”¨è¾“å‡ºæŠ•å½±
  fun t => applyLinear mha.W_o (summed t)

-- 13. å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰
def FFN :=
  { W1 : Linear EmbedDim HiddenDim
  , W2 : Linear HiddenDim EmbedDim
  , b1 : Vec HiddenDim
  , b2 : Vec EmbedDim
  }

def applyFFN (ffn : FFN) (x : Embedding) : Embedding :=
  let h := applyLinear ffn.W1 x + ffn.b1  -- çº¿æ€§ + åç½®
  let h_gelu := fun i => Real.gelu (h i)  -- GELU æ¿€æ´»ï¼ˆéœ€å¯¼å…¥æˆ–å®šä¹‰ï¼‰
  applyLinear ffn.W2 h_gelu + ffn.b2

-- 14. ç¼–ç å™¨å±‚
def EncoderLayer :=
  { mha : MultiHeadAttention
  , ffn : FFN
  , ln1 : LayerNorm EmbedDim
  , ln2 : LayerNorm EmbedDim
  }

-- 15. å®ç°ç¼–ç å™¨å±‚å‰å‘ä¼ æ’­
def forwardLayer (layer : EncoderLayer) 
                 (x : TokenCount â†’ Embedding) 
                 : TokenCount â†’ Embedding := 
  let x1 := fun t => applyLayerNorm layer.ln1 (x t + computeMultiHead layer.mha x t)
  let x2 := fun t => applyLayerNorm layer.ln2 (x1 t + applyFFN layer.ffn (x1 t))
  x2

-- 16. å‡è®¾ 6 å±‚ç¼–ç å™¨
abbrev LayerIdx := Fin 6

def TransformerEncoder :=
  LayerIdx â†’ EncoderLayer

def applyEncoder (encoder : TransformerEncoder) 
                 (x : TokenCount â†’ Embedding) 
                 : TokenCount â†’ Embedding := 
  let x_embed := fun t => x t  -- åˆå§‹åµŒå…¥ï¼ˆå®é™…ä¸­éœ€æŸ¥è¡¨ï¼‰
  let x_pos := fun t => fun d => x_embed t d + posEnc t d  -- åŠ ä½ç½®ç¼–ç 
  -- ç®€åŒ–ï¼šåªå®ç°ä¸€å±‚ï¼Œå¯é€’å½’å †å 
  forwardLayer (encoder 0) x_pos
  -- å¯æ‰©å±•ä¸ºé€’å½’ï¼šfoldl æˆ–é€’å½’å‡½æ•° over LayerIdx

theorem attention_weights_sum_to_one (head : SingleHeadAttention) 
                                     (x : TokenCount â†’ Embedding) 
                                     (t_out : TokenCount) :
    âˆ‘ t_in : TokenCount, (let scores := fun t1 t2 => ...; 
                          softmax (fun t2 => scores t_out t2)) t_in = 1 := by
  -- å±•å¼€ softmax å®šä¹‰ï¼Œåˆ©ç”¨ âˆ‘ exp / Z = Z/Z = 1
  simp [softmax]
  ring
  -- éœ€è¦è¯æ˜ Z â‰  0ï¼Œé€šå¸¸æˆç«‹

theorem residual_preserves_type :
    âˆ€ x : TokenCount â†’ Embedding,
       (fun t => x t + computeMultiHead mha x t) = 
       (some_other_expr t) â†’ 
       -- ç±»å‹è‡ªåŠ¨ä¿è¯ï¼šè¾“å‡ºä»æ˜¯ TokenCount â†’ Embedding
       True := by trivial



theorem emergence : 
  depth â‰¥ threshold â†’ âˆƒ few_shot_learning_ability := by admit

theorem phase_transition_in_scaling :
  let L := modelSize
  âˆƒ Lâ‚€, L â‰¥ Lâ‚€ â†’ generalizationGap drops sharply := by admit

def Understanding := 
  âˆ€ p : Prompt, f(p) â‰ˆ humanAnswerDistribution p

-- å®šä¹‰ï¼šTransformer æ¨¡å‹ç©ºé—´
universe u

structure Transformer (Input : Type) (Output : Type) where
  params : ParameterSpace
  forward : params â†’ Input â†’ Output
  config : {
    depth : â„•
    width : â„•
    vocabSize : â„•
    seqLen : â„•
  }

-- å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªé¢„è®­ç»ƒ + å¾®è°ƒæµç¨‹
def learns_from_data (f : Transformer Input Output) (ğ’Ÿ : Dataset) : Prop :=
  âˆƒ training_algorithm,
    training_algorithm f ğ’Ÿ â†’ f_trained âˆ§
    empiricalRisk f_trained ğ’Ÿ â‰¤ Îµ


let f := someLargeTransformer  -- å¦‚ 175B å‚æ•°çš„ GPT-scale æ¨¡å‹

theorem f_learns_from_data : learns_from_data f ğ’Ÿ := by
  -- ä½¿ç”¨æ ‡å‡†è®­ç»ƒç®—æ³•ï¼šAdamW + LR scheduling + dropout
  let training_algorithm := trainWithAdamW epochs := 100, lr := 3e-5, ...
  -- å·²çŸ¥ï¼šå¤§ Transformer åœ¨è¶³å¤Ÿæ•°æ®ä¸Šå¯è¾¾åˆ°ä½è®­ç»ƒè¯¯å·®
  -- å‚è§ï¼šBrown et al. (2020), "Language Models are Few-Shot Learners"
  have h_convergence : 
    training_algorithm f ğ’Ÿ â†’ f_trained âˆ§ empiricalRisk f_trained ğ’Ÿ â‰¤ 1e-3 := by
    -- ä¾èµ–ï¼šæŸå¤±å‡½æ•°å…‰æ»‘ã€æ¢¯åº¦æœ‰ç•Œã€å­¦ä¹ ç‡åˆé€‚
    -- å¯å½¢å¼åŒ–ä¸ºï¼šéšæœºæ¢¯åº¦ä¸‹é™åœ¨éå‡¸å‡½æ•°ä¸Šçš„æ”¶æ•›æ€§
    admit
  exact âŸ¨training_algorithm, h_convergenceâŸ©


theorem f_generalizes : generalizationGap f ğ’Ÿ â‰¤ O(âˆš(log d / n)) := by
  -- å¼•ç”¨æˆ‘ä»¬ä¹‹å‰çš„å½¢å¼åŒ–ç»“æœï¼š
  have h_rademacher_bound : 
    rademacherComplexity (TransfClass d L) â‰¤ C * d * L * log d := 
      rademacher_transformer_bound d L

  have h_generalization_bound : 
    generalizationGap f ğ’Ÿ â‰¤ 2 * rademacherComplexity â„‹ + O(âˆš(log(1/Î´)/n)) := 
      generalization_bound_via_rademacher â„‹ S Î´

  -- ç”±äº Transformer çš„å¤æ‚åº¦å¢é•¿ç¼“æ…¢ï¼ˆ`d*L*log d`ï¼‰
  -- ä¸” `n`ï¼ˆæ•°æ®é‡ï¼‰æå¤§æ—¶ï¼Œâˆš(log d / n) â†’ 0
  have h_small_gap : 
    n â‰¥ Nâ‚€ â†’ generalizationGap f ğ’Ÿ â‰¤ Îµ := by
      assume n h_n
      calc
        _ â‰¤ 2*C*d*L*log d + 3*âˆš(2*log(2/Î´)/n)
        _ â‰¤ Îµ  -- å½“ n è¶³å¤Ÿå¤§æ—¶æˆç«‹
        admit

  exact h_small_gap


def semanticallyEquivalent (pâ‚ pâ‚‚ : Prompt) : Prop :=
  âˆ€ human, humanInterpretation pâ‚ â‰ˆ humanInterpretation pâ‚‚

def understands (f : Transformer) : Prop :=
  âˆ€ pâ‚ pâ‚‚, semanticallyEquivalent pâ‚ pâ‚‚ â†’ f(pâ‚) â‰ˆ f(pâ‚‚)


-- å®šä¹‰ï¼šé€»è¾‘è•´å«
def entails (premise conclusion : Prop) : Prop := premise â†’ conclusion

def can_do_reasoning (f : Transformer) : Prop :=
  âˆ€ context, 
    let world := parseWorld context
    âˆ€ q, 
      let answer := f(context ++ "\nQ: " ++ q)
      (world âŠ¨ q) â†” (answer = "Yes")


def counterfactualRobustness (f : Transformer) (p : Prompt) (answer : Answer) : Prop :=
  âˆ€ perturbation, 
    semanticallyIrrelevant perturbation â†’
    f(p ++ perturbation) â‰ˆ answer

def understands (f : Transformer) : Prop :=
  âˆ€ p, 
    let a := f(p)
    counterfactualRobustness f p a


def Understanding :=
  understands_semantic f âˆ§
  understands_reasoning f âˆ§
  understands_counterfactual f âˆ§
  understands_emergent f  -- å¦‚æ€ç»´é“¾ã€è‡ªæˆ‘ä¿®æ­£


theorem f_understands : understands f := by
  -- è¯æ® 1ï¼šè¯­ä¹‰ä¸€è‡´æ€§
  have h_semantic : âˆ€ pâ‚ â‰¡ pâ‚‚, f(pâ‚) â‰ˆ f(pâ‚‚) := by
    -- å®éªŒæ”¯æŒï¼šå¤§æ¨¡å‹åœ¨ paraphrasing ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½
    -- å¦‚ï¼šGLUEã€SST-2
    admit

  -- è¯æ® 2ï¼šæ¨ç†ä¸€è‡´æ€§
  have h_reasoning : can_do_reasoning f := by
    -- å®éªŒï¼šå¤§æ¨¡å‹åœ¨æ•°å­¦ã€é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ° SOTA
    -- å¦‚ï¼šGSM8K, MATH, Logical Deduction
    admit

  -- è¯æ® 3ï¼šåäº‹å®é²æ£’æ€§
  have h_counterfactual : counterfactualRobustness f := by
    -- ç›¸å¯¹è¾ƒå¼±ï¼Œä½†é€šè¿‡æç¤ºå·¥ç¨‹å¯å¢å¼º
    admit

  -- è¯æ® 4ï¼šæ¶Œç°èƒ½åŠ›
  have h_emergent : f exhibits chain_of_thought âˆ§ f self_corrects := by
    -- å½“è§„æ¨¡è¶…è¿‡é˜ˆå€¼ï¼ŒCoTã€è‡ªæˆ‘ä¿®æ­£ç­‰èƒ½åŠ›â€œæ¶Œç°â€
    -- å‚è§ï¼šWei et al., "Chain-of-Thought Prompting Elicits Reasoning"
    admit

  exact âŸ¨h_semantic, h_reasoning, h_counterfactual, h_emergentâŸ©


theorem AI_is_possible : âˆƒ f, 
  learns_from_data f âˆ§ 
  generalizes f âˆ§ 
  understands f := by
  let f := someLargeTransformer
  apply Exists.intro f
  constructor
  Â· exact f_learns_from_data
  Â· exact f_generalizes
  Â· exact f_understands


theorem AGI_is_possible_under_scaling :
  let f := limit_{Lâ†’âˆ, dâ†’âˆ} Transformer
  âˆƒ capability_threshold, 
    size f â‰¥ threshold â†’ f exhibits general reasoning := by admit


theorem alignment_is_necessary :
  âˆƒ f, intelligent f â†’ âˆƒ risk, unaligned f â†’ catastrophe := by admit








import Mathlib
import Mathlib.Combinatorics.SimpleGraph
import Mathlib.Probability.ProbabilityMassFunction -- ç¦»æ•£æ¦‚ç‡
import Mathlib.Data.Fintype.Basic

open SimpleGraph

-- 1. å®šä¹‰å˜é‡ç±»å‹ï¼ˆå‡è®¾ç¦»æ•£ã€æœ‰é™ï¼‰
abbrev Var := Fin 10 -- 10ä¸ªå˜é‡ï¼Œç¼–å·0-9ã€‚å®é™…ä¸­å¯ç”¨å­—ç¬¦ä¸²æˆ–è‡ªå®šä¹‰ç±»å‹ã€‚

-- 2. å®šä¹‰è´å¶æ–¯ç½‘ç»œçš„å›¾ç»“æ„ï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰
-- åœ¨ Mathlib ä¸­ï¼ŒSimpleGraph é»˜è®¤æ˜¯æ— å‘çš„ã€‚æˆ‘ä»¬éœ€è¦æœ‰å‘å›¾ã€‚
-- Mathlib æœ‰ `Quiver`ï¼ˆå¹¿ä¹‰æœ‰å‘å›¾ï¼‰ï¼Œä½†æ›´å¸¸ç”¨çš„æ˜¯ç›´æ¥å®šä¹‰è¾¹é›†ã€‚
structure DirectedGraph (V : Type) where
  edges : V â†’ V â†’ Prop -- edges i j è¡¨ç¤ºå­˜åœ¨è¾¹ i â†’ j
  is_irreflexive : âˆ€ v, Â¬ edges v v -- æ— è‡ªç¯ï¼ˆå¯é€‰ï¼Œä½†DAGé€šå¸¸æ— è‡ªç¯ï¼‰
  is_acyclic : True -- ç®€åŒ–ï¼šå‡è®¾æ— ç¯ã€‚çœŸå®å½¢å¼åŒ–éœ€å®šä¹‰â€œæ— ç¯â€å¹¶è¯æ˜ã€‚è¿™æ˜¯éš¾ç‚¹ï¼

-- 3. å®šä¹‰å˜é‡çš„å–å€¼åŸŸï¼ˆå‡è®¾æ‰€æœ‰å˜é‡å–å€¼äºåŒä¸€ä¸ªæœ‰é™ç±»å‹ï¼Œç®€åŒ–ï¼‰
abbrev Value := Bool -- ä¾‹å¦‚ï¼Œå¸ƒå°”å˜é‡
instance : Fintype Value where
  fintype := inferInstance -- Bool æœ¬èº«æ˜¯ Fintype

-- 4. å®šä¹‰æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ (CPD)
-- å¯¹äºèŠ‚ç‚¹ vï¼Œå…¶ CPD ä¾èµ–äºå…¶çˆ¶èŠ‚ç‚¹çš„å–å€¼ã€‚
-- parents_vals : ä»çˆ¶èŠ‚ç‚¹åˆ°å…¶å–å€¼çš„æ˜ å°„
-- è¿”å›ä¸€ä¸ª PMFï¼Œè¡¨ç¤ºåœ¨ç»™å®šçˆ¶èŠ‚ç‚¹å–å€¼ä¸‹ï¼Œv çš„æ¦‚ç‡åˆ†å¸ƒã€‚
def CPD (G : DirectedGraph Var) (v : Var) :=
  (parents_vals : {p : Var | G.edges p v} â†’ Value) â†’ PMF Value
-- æ³¨æ„ï¼š{p : Var | G.edges p v} æ˜¯ v çš„çˆ¶èŠ‚ç‚¹é›†åˆï¼ˆå­ç±»å‹ï¼‰ã€‚ç”±äº Var æ˜¯ Fin nï¼Œè¿™ä¸ªé›†åˆæ˜¯æœ‰é™çš„ã€‚

-- 5. å®šä¹‰è´å¶æ–¯ç½‘ç»œç»“æ„
structure BayesianNetwork where
  graph : DirectedGraph Var
  cpds : (v : Var) â†’ CPD graph v -- ä¸ºæ¯ä¸ªèŠ‚ç‚¹æä¾›ä¸€ä¸ª CPD

-- 6. å®šä¹‰è”åˆæ¦‚ç‡åˆ†å¸ƒ
-- ç»™å®šä¸€ä¸ªå®Œæ•´çš„å˜é‡èµ‹å€¼ (assignment: Var â†’ Value)ï¼Œè®¡ç®—å…¶è”åˆæ¦‚ç‡ã€‚
def joint_prob (bn : BayesianNetwork) (assignment : Var â†’ Value) : â„ :=
  âˆ v : Var,
    let parents_vals := fun (p : {p : Var | bn.graph.edges p v}) => assignment p
    (bn.cpds v parents_vals) (assignment v) -- è·å– PMF åœ¨ assignment v å¤„çš„æ¦‚ç‡å€¼
-- æ³¨æ„ï¼šPMF Î± æ˜¯ä¸€ä¸ªä» Î± åˆ° â„ çš„å‡½æ•°ï¼Œæ»¡è¶³éè´Ÿä¸”å’Œä¸º1ã€‚æ‰€ä»¥ (pmf x) å°±æ˜¯æ¦‚ç‡å€¼ã€‚

-- 7. æ ¸å¿ƒæ€§è´¨ï¼šè”åˆæ¦‚ç‡éè´Ÿä¸”å’Œä¸º1
-- æ€§è´¨1ï¼šéè´Ÿæ€§
theorem joint_prob_nonneg
    (bn : BayesianNetwork)
    (assignment : Var â†’ Value)
    :
    0 â‰¤ joint_prob bn assignment := by
  -- ä¹˜ç§¯çš„æ¯ä¸€é¡¹ (bn.cpds v parents_vals) (assignment v) éƒ½ â‰¥ 0ï¼Œå› ä¸ºå®ƒæ˜¯ PMF çš„å€¼ã€‚
  -- Mathlib ä¸­æœ‰ PMF éè´Ÿçš„å®šç†ï¼šPMF.NonNeg
  simp [joint_prob]
  apply Finset.prod_nonneg -- æœ‰é™ä¹˜ç§¯çš„éè´Ÿæ€§
  intro v
  apply PMF.nonneg -- åº”ç”¨ PMF çš„éè´Ÿæ€§å®šç†

-- æ€§è´¨2ï¼šå½’ä¸€åŒ–ï¼ˆæ‰€æœ‰å¯èƒ½èµ‹å€¼çš„æ¦‚ç‡ä¹‹å’Œä¸º1ï¼‰
-- è¿™æ˜¯è´å¶æ–¯ç½‘ç»œå®šä¹‰çš„æ ¸å¿ƒï¼
theorem joint_prob_normalization
    (bn : BayesianNetwork)
    :
    âˆ‘ (assignment : Var â†’ Value) in Fintype.finset (Var â†’ Value), joint_prob bn assignment = 1 := by
  -- è¿™ä¸ªè¯æ˜ç›¸å¯¹å¤æ‚ï¼Œéœ€è¦ç”¨åˆ°â€œæŒ‰æ‹“æ‰‘åºåˆ†è§£æ±‚å’Œâ€çš„æ€æƒ³ã€‚
  -- ç”±äºå›¾æ˜¯ DAGï¼Œå­˜åœ¨æ‹“æ‰‘æ’åºã€‚æˆ‘ä»¬å¯ä»¥æŒ‰æ‹“æ‰‘åºå¯¹å˜é‡æ±‚å’Œã€‚
  -- ä¼ªè¯æ˜æ­¥éª¤ï¼š
  -- 1. å¯¹æ‹“æ‰‘åºä¸­çš„ç¬¬ä¸€ä¸ªå˜é‡ v1 æ±‚å’Œï¼šâˆ‘_{val1} P(v1) = 1ã€‚
  -- 2. å¯¹ç¬¬äºŒä¸ªå˜é‡ v2 æ±‚å’Œï¼šâˆ‘_{val2} P(v2 | parents(v2))ã€‚ç”±äº parents(v2) åªèƒ½æ˜¯ v1ï¼ˆæˆ–ç©ºï¼‰ï¼Œä¸” v1 å·²æ±‚å’Œï¼Œâˆ‘_{val2} P(v2 | val1) = 1ï¼Œæ‰€ä»¥æ•´ä½“è´¡çŒ®ä¸º 1 * 1ã€‚
  -- 3. ä¾æ­¤ç±»æ¨ã€‚
  -- åœ¨ Lean ä¸­ï¼Œè¿™éœ€è¦ï¼š
  --   a) å½¢å¼åŒ– DAG çš„æ‹“æ‰‘æ’åºã€‚
  --   b) ä½¿ç”¨ Fubini å®šç†ï¼ˆæˆ–ç¦»æ•£ç‰ˆæœ¬çš„æ±‚å’Œäº¤æ¢ï¼‰æŒ‰æ‹“æ‰‘åºé‡æ’æ±‚å’Œã€‚
  --   c) é€æ­¥åŒ–ç®€ï¼Œåˆ©ç”¨æ¯ä¸ª CPD çš„å½’ä¸€åŒ–æ€§è´¨ (âˆ‘_{val} P(val | parents_vals) = 1)ã€‚
  sorry -- è¿™æ˜¯ä¸€ä¸ªéå¹³å‡¡çš„è¯æ˜ï¼Œéœ€è¦å¤§é‡å‰ç½®å·¥ä½œã€‚



import Mathlib.Data.Matrix.Basic
import Mathlib.Analysis.Calculus.Deriv.FDeriv
import Mathlib.Data.Real.Basic

open Matrix
open scoped Matrix

/- 
  FlashAttention Backward çš„æ­£ç¡®æ€§è¯æ˜
  ç›®æ ‡ï¼šè¯æ˜å…¶è®¡ç®—çš„ (dQ, dK, dV) ç­‰äº loss å¯¹ (Q, K, V) çš„æ¢¯åº¦
-/

section FlashAttentionCorrectness

-- =============================================
-- 1. å‚æ•°ä¸ç±»å‹
-- =============================================

universe u

variable {N d : â„•}  -- åºåˆ—é•¿åº¦ï¼Œå¤´ç»´åº¦
variable [Fact (0 < N)] [Fact (0 < d)]

-- ç¼©æ”¾å› å­
variable (Ï„ : â„)  -- é€šå¸¸ä¸º 1/sqrt(d)

-- ç›®æ ‡è¾“å‡ºï¼ˆç”¨äºå®šä¹‰ lossï¼‰
variable (O_target : Matrix â„ N d)

-- =============================================
-- 2. å‰å‘ä¼ æ’­ï¼šO = softmax(Ï„ Q Káµ€) â€¢ V
-- =============================================

/-- softmax æŒ‰è¡Œå½’ä¸€åŒ– --/
def softmax (S : Matrix â„ N N) : Matrix â„ N N :=
  fun i j => exp (S i j) / âˆ‘ j' : Fin N, exp (S i j')

/-- å‰å‘ä¼ æ’­å‡½æ•°ï¼šè¾“å…¥ Q, K, Vï¼Œè¾“å‡º O --/
def forward (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  let S : Matrix â„ N N := Ï„ â€¢ (Q â¬ Káµ€)
  let P : Matrix â„ N N := softmax S
  P â¬ V

-- =============================================
-- 3. æŸå¤±å‡½æ•°ï¼šloss = â€–O - O_targetâ€–Â²
-- =============================================

/-- æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·® --/
def loss (Q K V : Matrix â„ N d) : â„ :=
  let O := forward Q K V
  âˆ‘ i j, (O i j - O_target i j)^2

-- =============================================
-- 4. æ‰‹åŠ¨æ¨å¯¼çš„æ¢¯åº¦ï¼ˆåå‘ä¼ æ’­å…¬å¼ï¼‰
-- =============================================

namespace ManualGradient

/-- 1. dO = 2*(O - O_target) --/
def dO (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  2 â€¢ (forward Q K V - O_target)

/-- 2. dV = Páµ€ â€¢ dO --/
def dV (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  (softmax (Ï„ â€¢ (Q â¬ Káµ€)))áµ€ â¬ dO Q K V

/-- 3. dP = dO â€¢ Váµ€ --/
def dP (Q K V : Matrix â„ N d) : Matrix â„ N N :=
  dO Q K V â¬ Váµ€

/-- 4. dS = dP âŠ™ P - P âŠ™ row_sum(dP âŠ™ P) --/
def dS (Q K V : Matrix â„ N d) : Matrix â„ N N :=
  let S := Ï„ â€¢ (Q â¬ Káµ€)
  let P := softmax S
  fun i j => dP Q K V i j * P i j - P i j * (âˆ‘ k, dP Q K V i k * P i k)

/-- 5. dQ = dS â€¢ K --/
def dQ (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  dS Q K V â¬ K

/-- 6. dK = dSáµ€ â€¢ Q --/
def dK (Q K V : Matrix â„ N d) : Matrix â„ N d :=
  (dS Q K V)áµ€ â¬ Q

end ManualGradient

-- =============================================
-- 5. FlashAttention Backward çš„å®ç°
-- =============================================

/-- æ¨¡æ‹Ÿ FlashAttention Backward çš„è¾“å‡º --/
def flash_backward (Q K V dO_input : Matrix â„ N d) :
    (Matrix â„ N d Ã— Matrix â„ N d Ã— Matrix â„ N d) :=
  let S := Ï„ â€¢ (Q â¬ Káµ€)
  let P := softmax S
  let dV := Páµ€ â¬ dO_input
  let dP := dO_input â¬ Váµ€
  let dS := fun i j => dP i j * P i j - P i j * (âˆ‘ k, dP i k * P i k)
  let dQ := dS â¬ K
  let dK := dSáµ€ â¬ Q
  (dQ, dK, dV)

-- =============================================
-- 6. æ­£ç¡®æ€§å®šç†
-- =============================================

/-- å®šç†ï¼šflash_backward åœ¨ dO_input = dO æ—¶ï¼Œè¿”å›æ‰‹åŠ¨æ¨å¯¼çš„æ¢¯åº¦ --/
theorem flash_backward_equals_manual (Q K V : Matrix â„ N d) :
    flash_backward Q K V (ManualGradient.dO Q K V) =
    (ManualGradient.dQ Q K V, ManualGradient.dK Q K V, ManualGradient.dV Q K V) :=
  by
    -- å±•å¼€ä¸¤è¾¹ï¼Œåˆ©ç”¨ dO_input = dO
    rfl

-- =============================================
-- 7. æ ¸å¿ƒè¯æ˜ï¼šæ‰‹åŠ¨æ¢¯åº¦ = è‡ªåŠ¨å¾®åˆ†
-- =============================================

/-- å®šç†ï¼šæ‰‹åŠ¨æ¨å¯¼çš„ dV ç­‰äº loss å¯¹ V çš„æ¢¯åº¦ --/
theorem manual_dV_eq_true_dV (Q K V : Matrix â„ N d) :
    ManualGradient.dV Q K V = (fun V => loss Q K V).fderiv_at V :=
  by
    -- 1. loss = â€–Pâ€¢V - O_targetâ€–Â²
    -- 2. d/dV â€–Aâ€¢V - Bâ€–Â² = 2 Aáµ€ (Aâ€¢V - B) = Aáµ€ â€¢ (2(O - B)) = Páµ€ â€¢ dO
    -- 3. å› æ­¤ âˆ‡_V loss = Páµ€ â€¢ dO
    have : (fun V => loss Q K V) = fun V => â€–(softmax (Ï„ â€¢ (Q â¬ Káµ€)) â¬ V) - O_targetâ€–Â² := rfl
    rw [this]
    -- ä½¿ç”¨çŸ©é˜µå¯¼æ•°å¼•ç†
    apply fderiv_mse_of_linear
    Â· apply continuous_linear_map.has_fderiv_at
    Â· rw [ManualGradient.dO]
      exact (fun V => 2 â€¢ ((softmax (Ï„ â€¢ (Q â¬ Káµ€)) â¬ V) - O_target))
    done

/-- å®šç†ï¼šæ‰‹åŠ¨æ¨å¯¼çš„ dQ ç­‰äº loss å¯¹ Q çš„æ¢¯åº¦ --/
theorem manual_dQ_eq_true_dQ (Q K V : Matrix â„ N d) :
    ManualGradient.dQ Q K V = (fun Q => loss Q K V).fderiv_at Q :=
  by
    -- é“¾å¼æ³•åˆ™ï¼šloss â†’ O â†’ P â†’ S â†’ Q
    -- 1. dO = 2(O - O_target)
    -- 2. dP = dO â€¢ Váµ€
    -- 3. dS = softmax_grad(P, dP)
    -- 4. dQ = dS â€¢ K
    -- 5. å› ä¸º S = Ï„ Q Káµ€ â†’ dS/dQ = Î» Î´Q => Ï„ Î´Q Káµ€ â†’ âŸ¨dS, dS/dQ(Î´Q)âŸ© = tr(dSáµ€ Ï„ Î´Q Káµ€) = tr((Ï„ dS K)áµ€ Î´Q)
    -- 6. æ‰€ä»¥ âˆ‡_Q = Ï„ dS Kï¼Œä½† dS å·²å« Ï„ï¼ˆå›  S = Ï„ Q Káµ€ï¼‰ï¼Œæ•… âˆ‡_Q = dS K
    apply chain_rule_two_steps
    Â· apply ManualGradient.dS_correct  -- å‡è®¾æœ‰ softmax åå‘å¼•ç†
    Â· apply has_fderiv_at.comp
      Â· apply fderiv_of_matrix_mul_const_on_left
      Â· apply fderiv_const_mul_matrix_on_right
    done

/-- åŒç† dK --/
theorem manual_dK_eq_true_dK (Q K V : Matrix â„ N d) :
    ManualGradient.dK Q K V = (fun K => loss Q K V).fderiv_at K :=
  by
    -- ç±»ä¼¼ dQï¼Œä½†æ–¹å‘ä¸åŒ
    skip

/-- ä¸»å®šç†ï¼šFlashAttention Backward è®¡ç®—çš„æ˜¯çœŸæ­£çš„æ¢¯åº¦ --/
theorem flash_backward_is_correct (Q K V : Matrix â„ N d) :
    flash_backward Q K V (ManualGradient.dO Q K V) =
      ((fun Q => loss Q K V).fderiv_at Q,
       (fun K => loss Q K V).fderiv_at K,
       (fun V => loss Q K V).fderiv_at V) :=
  by
    rw [flash_backward_equals_manual]
    rw [manual_dQ_eq_true_dQ]
    rw [manual_dK_eq_true_dK]
    rw [manual_dV_eq_true_dV]
    done

end FlashAttentionCorrectness


import Mathlib
import Mathlib.Data.Fintype.Basic
import Mathlib.Data.Finset
import Mathlib.Algebra.BigOperators.Basic
import Mathlib.Analysis.SpecialFunctions.ExpLog

-- 1. å®šä¹‰åŸºæœ¬ç±»å‹
abbrev StateIdx := Fin 3 -- å‡è®¾æœ‰3ä¸ªéšçŠ¶æ€: s0, s1, s2
abbrev ObsIdx := Fin 2   -- å‡è®¾æœ‰2ç§è§‚æµ‹å€¼: v0, v1
abbrev TimeIdx := Nat    -- æ—¶é—´ç´¢å¼•ï¼Œä»1å¼€å§‹

-- è§‚æµ‹åºåˆ— (å›ºå®šé•¿åº¦ T)
abbrev ObsSeq (T : Nat) := Fin T â†’ ObsIdx

-- 2. å®šä¹‰ HMM å‚æ•°
structure HMMParams where
  pi : StateIdx â†’ â„â‰¥0 -- åˆå§‹æ¦‚ç‡
  pi_sum : âˆ‘ i, pi i = 1 -- åˆå§‹æ¦‚ç‡å’Œä¸º1
  a : StateIdx â†’ StateIdx â†’ â„â‰¥0 -- è½¬ç§»æ¦‚ç‡ a[i][j] = P(q_{t+1}=j | q_t=i)
  a_sum : âˆ€ i, âˆ‘ j, a i j = 1 -- è½¬ç§»æ¦‚ç‡è¡Œå’Œä¸º1
  b : StateIdx â†’ ObsIdx â†’ â„â‰¥0 -- å‘å°„æ¦‚ç‡ b[i][k] = P(o_t=k | q_t=i)
  b_sum : âˆ€ i, âˆ‘ k, b i k = 1 -- å‘å°„æ¦‚ç‡è¡Œå’Œä¸º1 (å¯é€‰ï¼Œæœ‰æ—¶å‘å°„æ¦‚ç‡ä¸å¼ºåˆ¶å’Œä¸º1)

-- 3. å®šä¹‰çŠ¶æ€åºåˆ— (é•¿åº¦ä¸º t)
abbrev StateSeq (t : Nat) := Fin t â†’ StateIdx

-- 4. ç›´æ¥å®šä¹‰ï¼šå‰å‘æ¦‚ç‡ Î±_t(i) (åŸºäºæ‰€æœ‰å¯èƒ½çš„å‰ t-1 ä¸ªçŠ¶æ€åºåˆ—)
-- P(o_1..o_t, q_t = s_i | Î»)
def forward_direct (Î» : HMMParams) (O : ObsSeq T) (t : Fin T) (i : StateIdx) : â„â‰¥0 :=
  if h_t_eq_zero : t.val = 0 then
    -- t=1 (å› ä¸º Fin T ä»0å¼€å§‹ï¼Œt.val=0 å¯¹åº”æ—¶åˆ»1)
    Î».pi i * Î».b i (O t)
  else
    -- t > 1: å¯¹æ‰€æœ‰å¯èƒ½çš„å‰ t ä¸ªçŠ¶æ€åºåˆ—æ±‚å’Œï¼Œè¦æ±‚ç¬¬ t ä¸ªçŠ¶æ€æ˜¯ i
    âˆ‘ (Q : StateSeq (t.val + 1)) in Fintype.finset (StateSeq (t.val + 1)), 
      if h_last_state : Q t = i then
        -- è®¡ç®—è·¯å¾„æ¦‚ç‡: Ï€ * a * b
        let path_prob : â„â‰¥0 :=
          -- åˆå§‹æ¦‚ç‡
          Î».pi (Q 0) *
          -- è½¬ç§»æ¦‚ç‡ä¹˜ç§¯ (ä»æ—¶åˆ»1åˆ°t-1)
          (âˆ s : Fin t.val, Î».a (Q âŸ¨s, by decideâŸ©) (Q âŸ¨s + 1, by have := s.is_lt; omegaâŸ©)) *
          -- å‘å°„æ¦‚ç‡ä¹˜ç§¯ (ä»æ—¶åˆ»1åˆ°t)
          (âˆ s : Fin (t.val + 1), Î».b (Q s) (O âŸ¨s, by decideâŸ©))
        path_prob
      else
        0

-- 5. é€’æ¨å®šä¹‰ï¼šå‰å‘æ¦‚ç‡ Î±_t(i)
-- æˆ‘ä»¬ç”¨ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ‰€æœ‰æ—¶åˆ»å’Œæ‰€æœ‰çŠ¶æ€çš„å‰å‘æ¦‚ç‡ã€‚
-- è¿”å›ä¸€ä¸ªäºŒç»´æ•°ç»„: [æ—¶åˆ» t][çŠ¶æ€ i] -> Î±_t(i)
def forward_recursive (Î» : HMMParams) (O : ObsSeq T) : (t : Fin T) â†’ StateIdx â†’ â„â‰¥0 := 
  let rec go : (t : Nat) â†’ StateIdx â†’ â„â‰¥0
    | 0, i => -- æ—¶åˆ»1 (t=0)
      Î».pi i * Î».b i (O 0)
    | t'+1, j => -- æ—¶åˆ» t'+2
      âˆ‘ i : StateIdx, 
        go t' i * Î».a i j * Î».b j (O âŸ¨t'+1, by decideâŸ©)
  fun t => go t.val

-- 6. æ ¸å¿ƒå®šç†ï¼šè¯æ˜é€’æ¨å®šä¹‰ä¸ç›´æ¥å®šä¹‰ç­‰ä»·
theorem forward_equivalence
    (Î» : HMMParams)
    (O : ObsSeq T)
    (t : Fin T)
    (i : StateIdx)
    :
    forward_recursive Î» O t i = forward_direct Î» O t i := by
  -- å¯¹ t è¿›è¡Œå½’çº³ (å®é™…ä¸Šæ˜¯ t.val çš„å½’çº³)
  induction t.val with
  | zero => 
    -- åŸºç¡€æƒ…å†µ: t.val = 0 (å¯¹åº”æ—¶åˆ»1)
    simp [forward_recursive, forward_direct]
    -- ä¸¤è¾¹éƒ½ç­‰äº Î».pi i * Î».b i (O 0)
    rfl
  | succ t' ih => 
    -- å½’çº³æ­¥éª¤: å‡è®¾å¯¹ t' æˆç«‹ï¼Œè¯æ˜å¯¹ t'+1 æˆç«‹
    -- ih æ˜¯å½’çº³å‡è®¾: âˆ€ i, forward_recursive Î» O âŸ¨t', _âŸ© i = forward_direct Î» O âŸ¨t', _âŸ© i
    simp [forward_recursive] -- å±•å¼€é€’æ¨å®šä¹‰
    -- ç›®æ ‡ï¼šâˆ‘ i, forward_recursive Î» O âŸ¨t', _âŸ© i * a i j * b j (O âŸ¨t'+1, _âŸ©) = forward_direct Î» O âŸ¨t'+1, _âŸ© j
    rw [ih] -- åº”ç”¨å½’çº³å‡è®¾ï¼Œå°† forward_recursive æ›¿æ¢ä¸º forward_direct
    -- ç›®æ ‡ï¼šâˆ‘ i, forward_direct Î» O âŸ¨t', _âŸ© i * a i j * b j (O âŸ¨t'+1, _âŸ©) = forward_direct Î» O âŸ¨t'+1, _âŸ© j
    -- ç°åœ¨éœ€è¦å±•å¼€ forward_direct Î» O âŸ¨t'+1, _âŸ© j çš„å®šä¹‰
    dsimp [forward_direct] -- å±•å¼€ç›´æ¥å®šä¹‰
    -- ç”±äº t'+1 > 0ï¼Œä¼šè¿›å…¥ else åˆ†æ”¯
    -- ç›®æ ‡ï¼šâˆ‘ Q in all_seqs, if Q (t'+1) = j then path_prob else 0 = âˆ‘ i, forward_direct Î» O âŸ¨t', _âŸ© i * a i j * b j (O âŸ¨t'+1, _âŸ©)
    -- å…³é”®ï¼šå°†å¯¹é•¿åº¦ä¸º t'+2 çš„åºåˆ— Q çš„æ±‚å’Œï¼Œåˆ†è§£ä¸ºï¼š
    --   1. å¯¹æ—¶åˆ» t'+1 çš„çŠ¶æ€ i æ±‚å’Œã€‚
    --   2. å¯¹å‰ t'+1 ä¸ªçŠ¶æ€ï¼ˆæ„æˆä¸€ä¸ªé•¿åº¦ä¸º t'+1 çš„åºåˆ— Q'ï¼‰æ±‚å’Œã€‚
    -- è¿™éœ€è¦ Fubini å®šç†ï¼ˆæ±‚å’Œäº¤æ¢ï¼‰æˆ–æ‰‹åŠ¨æ„é€ åŒå°„ã€‚
    sorry -- è¿™æ˜¯è¯æ˜çš„æ ¸å¿ƒéš¾ç‚¹ï¼Œéœ€è¦è¯¦ç»†å±•å¼€è·¯å¾„æ±‚å’Œã€‚



-- é€šå¸¸ï¼Œå½¢å¼åŒ–é¡¹ç›®ä¼šæ”¾åœ¨ä¸€ä¸ªå‘½åç©ºé—´å†…
import Mathlib -- å¯¼å…¥æ•´ä¸ª Mathlib4ï¼Œæˆ–æŒ‰éœ€å¯¼å…¥ç‰¹å®šæ¨¡å—
import Mathlib.Data.Fintype.Basic -- æœ‰é™ç±»å‹
import Mathlib.Data.Finset -- æœ‰é™é›†åˆ
import Mathlib.Algebra.BigOperators.Basic -- æ±‚å’Œç¬¦å· âˆ‘
import Mathlib.Analysis.SpecialFunctions.ExpLog -- æŒ‡æ•°å‡½æ•°
import Mathlib.Tactic.FieldSimp -- field_simp tactic

-- ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾ï¼š
-- 1. æ ‡ç­¾é›† `Label` æ˜¯ä¸€ä¸ªæœ‰é™ç±»å‹ (Fintype)ã€‚
-- 2. åºåˆ—é•¿åº¦ `n` æ˜¯å›ºå®šçš„ã€‚
-- 3. ç‰¹å¾å‡½æ•°å’Œæƒé‡æ˜¯ç»™å®šçš„ã€‚
-- 4. æˆ‘ä»¬å…³æ³¨çš„æ˜¯ç»™å®šè§‚æµ‹ `x` ä¸‹ï¼Œæ‰€æœ‰é•¿åº¦ä¸º `n` çš„æ ‡ç­¾åºåˆ—çš„å½’ä¸€åŒ–ã€‚

-- å®šä¹‰æ ‡ç­¾ç±»å‹ (å‡è®¾æ˜¯æœ‰é™çš„)
abbrev Label := Fin 5 -- ä¾‹å¦‚ï¼Œ5ä¸ªæ ‡ç­¾ {0, 1, 2, 3, 4}ã€‚åœ¨çœŸå®é¡¹ç›®ä¸­å¯èƒ½æ˜¯ `String` æˆ–è‡ªå®šä¹‰inductiveç±»å‹ï¼Œä½†éœ€è¯æ˜å…¶Fintypeå®ä¾‹ã€‚
-- @[derive Fintype] -- å¦‚æœæ˜¯è‡ªå®šä¹‰ç±»å‹ï¼Œå¯ä»¥ç”¨è¿™ä¸ªè‡ªåŠ¨ç”ŸæˆFintypeå®ä¾‹

-- å®šä¹‰é•¿åº¦ä¸º n çš„æ ‡ç­¾åºåˆ—ç±»å‹
-- åœ¨ Mathlib4 ä¸­ï¼Œå¯¹äºå›ºå®šé•¿åº¦çš„åºåˆ—ï¼Œå¸¸ç”¨ `Vector Î± n` æˆ– `Fin n â†’ Î±`ã€‚
-- è¿™é‡Œç”¨ `Fin n â†’ Label`ï¼Œå®ƒè¡¨ç¤ºä»ä½ç½® {0, 1, ..., n-1} åˆ°æ ‡ç­¾çš„å‡½æ•°ã€‚
abbrev LabelSeq (n : Nat) := Fin n â†’ Label

-- å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè§‚æµ‹åºåˆ—ç±»å‹ (è¿™é‡Œç®€åŒ–ä¸º Unitï¼Œå› ä¸ºæˆ‘ä»¬å…³æ³¨çš„æ˜¯ç»™å®š x çš„æ¡ä»¶æ¦‚ç‡)
abbrev ObsSeq := Unit -- åœ¨çœŸå®æ¨¡å‹ä¸­ï¼Œè¿™ä¼šæ˜¯ List Char æˆ– Vector Word n ç­‰å¤æ‚ç±»å‹ã€‚

-- å®šä¹‰ç‰¹å¾å‡½æ•°ç±»å‹ (ç®€åŒ–ç‰ˆ)
-- ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ç‰¹å¾å‡½æ•°åªä¾èµ–äºç›¸é‚»æ ‡ç­¾å’Œä½ç½®ï¼Œä¸”ç‰¹å¾æ•°é‡æ˜¯å›ºå®šçš„ã€‚
-- f : (å‰ä¸€ä¸ªæ ‡ç­¾) â†’ (å½“å‰æ ‡ç­¾) â†’ (ä½ç½® i) â†’ Real
abbrev FeatureFunc := Label â†’ Label â†’ Fin n â†’ â„

-- å®šä¹‰æƒé‡å‘é‡ (ç®€åŒ–ç‰ˆï¼Œå‡è®¾ä¸€ä¸ªç‰¹å¾å‡½æ•°)
-- åœ¨çœŸå®æ¨¡å‹ä¸­ï¼Œä¼šæ˜¯ FeatureFunc çš„ç´¢å¼•åˆ° â„ çš„æ˜ å°„ã€‚
abbrev Weight := â„

-- è®¡ç®—å•ä¸ªä½ç½® i çš„â€œå±€éƒ¨å¾—åˆ†â€ (ç®€åŒ–ï¼šåªç”¨ä¸€ä¸ªç‰¹å¾å‡½æ•°å’Œæƒé‡)
-- æ³¨æ„ï¼šå¯¹äº i=0ï¼Œæ²¡æœ‰ y_{i-1}ï¼Œé€šå¸¸éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆå¦‚ START æ ‡ç­¾ï¼‰ã€‚è¿™é‡Œæˆ‘ä»¬å‡è®¾ i > 0ï¼Œæˆ–å®šä¹‰ä¸€ä¸ªè™šæ‹Ÿçš„èµ·å§‹æ ‡ç­¾ã€‚
-- ä¸ºäº†ç®€åŒ–è¯æ˜ï¼Œæˆ‘ä»¬å‡è®¾åºåˆ—ä» i=1 å¼€å§‹è®¡ç®—è½¬ç§»ï¼Œå¹¶å¿½ç•¥ i=0 çš„å‘å°„ç‰¹å¾ã€‚
def local_score (w : Weight) (f : FeatureFunc) (y : LabelSeq n) (i : Fin (n - 1)) : â„ :=
  -- i : Fin (n-1) è¡¨ç¤ºä½ç½® 0 åˆ° n-2ï¼Œå¯¹åº”è½¬ç§» (y i) -> (y (i+1))
  w * f (y i) (y (i.succ)) i
-- æ³¨æ„ï¼ši.succ : Fin nï¼Œå› ä¸º i < n-1, æ‰€ä»¥ i+1 < n.

-- è®¡ç®—æ•´ä¸ªåºåˆ—çš„å¾—åˆ† (å¯¹æ‰€æœ‰è½¬ç§»ä½ç½®æ±‚å’Œ)
-- æˆ‘ä»¬éœ€è¦å¯¹ Fin (n-1) ä¸Šçš„æ‰€æœ‰ i æ±‚å’Œã€‚
def score (w : Weight) (f : FeatureFunc) (y : LabelSeq n) : â„ :=
  âˆ‘ i : Fin (n - 1), local_score w f y i
-- Mathlib4 çš„ âˆ‘ æ˜¯ Finset.sum æˆ– Fintype.sum çš„è¯­æ³•ç³–ã€‚è¿™é‡Œå› ä¸º Fin (n-1) æ˜¯ Fintypeï¼Œæ‰€ä»¥å¯ä»¥è¿™æ ·ç”¨ã€‚

-- å®šä¹‰æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾åºåˆ—çš„é›†åˆ
-- å› ä¸º Label æ˜¯ Fintypeï¼ŒFin n ä¹Ÿæ˜¯ Fintypeï¼Œæ‰€ä»¥ LabelSeq n = (Fin n â†’ Label) ä¹Ÿæ˜¯ Fintypeã€‚
-- æˆ‘ä»¬å¯ä»¥è·å–å…¶æ‰€æœ‰å…ƒç´ çš„ Finsetã€‚
def all_label_seqs (n : Nat) [Fintype Label] : Finset (LabelSeq n) :=
  Fintype.finset (LabelSeq n) -- åˆ©ç”¨ Fintype å®ä¾‹è‡ªåŠ¨ç”Ÿæˆæœ‰é™é›†åˆ

-- å®šä¹‰é…åˆ†å‡½æ•° Z(x) (è¿™é‡Œ x æ˜¯ Unitï¼Œæ‰€ä»¥ Z ä¸ä¾èµ–äº x)
-- Z = Î£_{y âˆˆ all_label_seqs} exp(score(y))
def partition_fn (n : Nat) [Fintype Label] (w : Weight) (f : FeatureFunc) : â„ :=
  âˆ‘ y in all_label_seqs n, Real.exp (score w f y)
-- è¿™é‡Œ âˆ‘ y in finset, ... æ˜¯ Finset.sum çš„è¯­æ³•ç³–ã€‚

-- å®šä¹‰æ¡ä»¶æ¦‚ç‡ P(y|x) (x æ˜¯ Unit)
def cond_prob (n : Nat) [Fintype Label] (w : Weight) (f : FeatureFunc) (y : LabelSeq n) : â„ :=
  if h : partition_fn n w f â‰  0 then
    Real.exp (score w f y) / partition_fn n w f
  else
    0 -- æˆ–è€…å®šä¹‰ä¸ºæœªå®šä¹‰ï¼Œä½†åœ¨è¯æ˜å½’ä¸€åŒ–æ—¶æˆ‘ä»¬å‡è®¾ Zâ‰ 0
-- åœ¨æ¦‚ç‡æ¨¡å‹ä¸­ï¼ŒZ é€šå¸¸æ˜¯æ­£æ•°ï¼Œå› ä¸º exp(score) > 0 ä¸”è‡³å°‘æœ‰ä¸€ä¸ªåºåˆ—ã€‚

-- æ ¸å¿ƒå®šç†ï¼šå½’ä¸€åŒ–æ€§è´¨
-- å¯¹äºä»»ä½•æƒé‡ wã€ç‰¹å¾å‡½æ•° f å’Œåºåˆ—é•¿åº¦ n (n > 0)ï¼Œæ‰€æœ‰å¯èƒ½æ ‡ç­¾åºåˆ—çš„æ¦‚ç‡ä¹‹å’Œä¸º 1ã€‚
-- æˆ‘ä»¬éœ€è¦å‡è®¾ partition_fn â‰  0ã€‚
theorem crf_normalization
    (n : Nat)
    [Fintype Label]
    (w : Weight)
    (f : FeatureFunc)
    (h_n_pos : n > 0) -- ç¡®ä¿åºåˆ—æœ‰é•¿åº¦ï¼Œè½¬ç§»ä½ç½® Fin (n-1) éç©ºæˆ–è‡³å°‘æœ‰ä¸€ä¸ªåºåˆ—
    (h_z_nonzero : partition_fn n w f â‰  0)
    :
    âˆ‘ y in all_label_seqs n, cond_prob n w f y = 1 := by
  -- å±•å¼€ cond_prob çš„å®šä¹‰ã€‚ç”±äºæˆ‘ä»¬æœ‰ h_z_nonzero å‡è®¾ï¼Œä¼šèµ° if çš„ then åˆ†æ”¯ã€‚
  simp only [cond_prob, h_z_nonzero] -- simp ä¼šå°è¯•åŒ–ç®€ï¼Œåˆ©ç”¨ h_z_nonzero é€‰æ‹© then åˆ†æ”¯
  -- ç°åœ¨ç›®æ ‡æ˜¯ï¼šâˆ‘ y in all_label_seqs n, (Real.exp (score w f y) / partition_fn n w f) = 1

  -- å°†å¸¸æ•° 1 / (partition_fn n w f) æå–åˆ°æ±‚å’Œç¬¦å·å¤–ã€‚
  -- ä½¿ç”¨ Finset.sum_mul ç³»åˆ—å®šç†ã€‚å…·ä½“æ˜¯ Finset.sum_const_mul æˆ–ç±»ä¼¼ã€‚
  -- æŸ¥æ‰¾å®šç†ï¼šâˆ‘ x in s, c * f x = c * âˆ‘ x in s, f x
  rw [Finset.sum_mul_left] -- æˆ–è€…å¯èƒ½æ˜¯ Finset.sum_smulï¼Œå–å†³äºå…·ä½“ç±»å‹ã€‚è¿™é‡Œå‡è®¾æ˜¯ä¹˜æ³•ã€‚
  -- ç›®æ ‡å˜ä¸ºï¼š(1 / partition_fn n w f) * (âˆ‘ y in all_label_seqs n, Real.exp (score w f y)) = 1

  -- æ ¹æ® partition_fn çš„å®šä¹‰ï¼Œâˆ‘ y in all_label_seqs n, Real.exp (score w f y) å°±æ˜¯ partition_fn n w f
  rw [partition_fn] -- ç›´æ¥ä»£å…¥å®šä¹‰
  -- ç›®æ ‡å˜ä¸ºï¼š(1 / partition_fn n w f) * (partition_fn n w f) = 1

  -- ä½¿ç”¨ field_simp åŒ–ç®€å®æ•°é™¤æ³•å’Œä¹˜æ³•
  field_simp [h_z_nonzero] -- è¿™ä¼šè‡ªåŠ¨åº”ç”¨ (1/a) * a = 1 (å½“ a â‰  0)
  -- è¯æ˜å®Œæˆ
  rfl -- æœ€ç»ˆç›®æ ‡æ˜¯ 1 = 1


import Mathlib.Data.Matrix.Basic
import Mathlib.Data.Finset.Basic
import Mathlib.Algebra.Group.Defs
import Mathlib.Data.Real.Basic

-- å‡è®¾æˆ‘ä»¬æœ‰åŸºæœ¬çš„çŸ©é˜µè¿ç®—æ”¯æŒ
open Matrix

/- 
  FlashAttention Backward Pass çš„å½¢å¼åŒ–ç»“æ„
  ç›®æ ‡ï¼šéªŒè¯ dQ, dK, dV çš„è®¡ç®—é€»è¾‘è„‰ç»œ
-/

section FlashAttentionBackward

-- =============================================
-- å‚æ•°ä¸ç±»å‹å®šä¹‰
-- =============================================

universe u

-- åºåˆ—é•¿åº¦ã€å¤´ç»´åº¦ã€å—å¤§å°
variable {N d : â„•}  -- N: seq len, d: head dim
variable {Bc Br : â„•}  -- Bc: key/value block size, Br: query block size

-- çŸ©é˜µç±»å‹åˆ«å
def QType := Matrix â„ N d
def KType := Matrix â„ N d
def VType := Matrix â„ N d
def OType := Matrix â„ N d
def dOType := Matrix â„ N d
def dQType := Matrix â„ N d
def dKType := Matrix â„ N d
def dVType := Matrix â„ N d

-- æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ
def SType := Matrix â„ N N
def PType := Matrix â„ N N
def ZType := Matrix â„ N N  -- dropout mask

-- ç¼©æ”¾å› å­
variable (Ï„ : â„)  -- usually 1/sqrt(d_k)

-- Mask å‡½æ•°ï¼šè¾“å…¥æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¾“å‡º masked åˆ†æ•°
variable (mask : SType â†’ SType)

-- Dropout æ¦‚ç‡
variable (p_drop : â„)  -- 0 â‰¤ p_drop < 1

-- =============================================
-- å­é—®é¢˜ 1ï¼šå‰å‘ä¼ æ’­å›é¡¾ï¼ˆä¸ºåå‘æä¾›ä¾èµ–ï¼‰
-- =============================================

namespace Forward

/-- å‰å‘ï¼šè®¡ç®— S = Q K^T --/
def computeS (Q : QType) (K : KType) : SType :=
  Q â¬ Káµ€

/-- å‰å‘ï¼šåº”ç”¨ mask --/
def applyMask (S : SType) : SType :=
  mask S

/-- å‰å‘ï¼šè®¡ç®— P = softmax(S_masked) --/
def softmax (S : SType) : PType :=
  fun i j => exp (S i j) / âˆ‘ j', exp (S i j')

/-- å‰å‘ï¼šè®¡ç®— O = P â€¢ V --/
def computeO (P : PType) (V : VType) : OType :=
  P â¬ V

end Forward

-- =============================================
-- å­é—®é¢˜ 2ï¼šåå‘ä¼ æ’­ä¸»é€»è¾‘
-- =============================================

namespace Backward

/-- åå‘ä¼ æ’­çš„ç›®æ ‡ï¼šç»™å®š dO, Q, K, V, P, è¿”å› dQ, dK, dV --/
def backwardPass (dO : dOType) (Q : QType) (K : KType) (V : VType) (P : PType) :
    (dQType Ã— dKType Ã— dVType) :=
  sorry  -- å®ç°å°†åœ¨â€œå­é—®é¢˜åˆ‡åˆ†â€ä¸­å±•å¼€

-- =============================================
-- å­é—®é¢˜ 2.1ï¼šè®¡ç®— dV
-- ä¾èµ–ï¼šdO, P
-- å…¬å¼ï¼šdV = P^T â€¢ dO
-- =============================================

namespace D_V

/-- å®šç†ï¼šdV çš„è®¡ç®—å…¬å¼ --/
theorem dV_formula (dO : dOType) (P : PType) : 
    dVType := Páµ€ â¬ dO

/-- æ­£ç¡®æ€§å¼•ç†ï¼šdV æ˜¯ loss å¯¹ V çš„æ¢¯åº¦ --/
theorem dV_correctness (L : â„) (V : VType) (dO : dOType) (P : PType) :
    âˆ‡_V L = dV_formula dO P :=
  sorry  -- éœ€è¦è‡ªåŠ¨å¾®åˆ†æˆ–é“¾å¼æ³•åˆ™æ”¯æŒ

end D_V

-- =============================================
-- å­é—®é¢˜ 2.2ï¼šè®¡ç®— dP
-- ä¾èµ–ï¼šdO, V
-- å…¬å¼ï¼šdP = dO â€¢ V^T
-- =============================================

namespace D_P

/-- è®¡ç®— dP --/
def compute_dP (dO : dOType) (V : VType) : PType :=
  dO â¬ Váµ€

/-- æ­£ç¡®æ€§ï¼šdP æ˜¯ loss å¯¹ P çš„æ¢¯åº¦ --/
theorem dP_correctness (L : â„) (P : PType) (dO : dOType) (V : VType) :
    âˆ‡_P L = compute_dP dO V :=
  sorry

end D_P

-- =============================================
-- å­é—®é¢˜ 2.3ï¼šè®¡ç®— dS
-- ä¾èµ–ï¼šdP, P
-- å…¬å¼ï¼šdS = dP * P - P * (dP * P) è¡Œæ±‚å’Œ
-- å³ï¼šdS_ij = dP_ij * P_ij - P_ij * Î£_k dP_ik P_ik
-- =============================================

namespace D_S

/-- è®¡ç®— dSï¼Œsoftmax çš„åå‘ --/
def compute_dS (dP : PType) (P : PType) : SType :=
  fun i j =>
    dP i j * P i j - P i j * (âˆ‘ k, dP i k * P i k)

/-- æ­£ç¡®æ€§ï¼šdS æ˜¯ loss å¯¹ S çš„æ¢¯åº¦ --/
theorem dS_correctness (L : â„) (S : SType) (dP : PType) (P : PType) :
    âˆ‡_S L = compute_dS dP P :=
  sorry

end D_S

-- =============================================
-- å­é—®é¢˜ 2.4ï¼šè®¡ç®— dQ å’Œ dK
-- ä¾èµ–ï¼šdS, Q, K
-- å…¬å¼ï¼šdQ = dS â€¢ K, dK = dS^T â€¢ Q
-- =============================================

namespace D_QK

/-- è®¡ç®— dQ --/
def compute_dQ (dS : SType) (K : KType) : QType :=
  dS â¬ K

/-- è®¡ç®— dK --/
def compute_dK (dS : SType) (Q : QType) : KType :=
  dSáµ€ â¬ Q

/-- æ­£ç¡®æ€§ï¼šdQ æ˜¯ loss å¯¹ Q çš„æ¢¯åº¦ --/
theorem dQ_correctness (L : â„) (Q : QType) (dS : SType) (K : KType) :
    âˆ‡_Q L = compute_dQ dS K :=
  sorry

/-- æ­£ç¡®æ€§ï¼šdK æ˜¯ loss å¯¹ K çš„æ¢¯åº¦ --/
theorem dK_correctness (L : â„) (K : KType) (dS : SType) (Q : QType) :
    âˆ‡_K L = compute_dK dS Q :=
  sorry

end D_QK

-- =============================================
-- å­é—®é¢˜ 2.5ï¼šæ•´åˆæ‰€æœ‰æ¢¯åº¦
-- =============================================

/-- ä¸»åå‘å‡½æ•°ï¼šæ•´åˆæ‰€æœ‰å­é—®é¢˜ --/
theorem backwardPass_def (dO : dOType) (Q : QType) (K : KType) (V : VType) (P : PType) :
    backwardPass dO Q K V P =
      let dV := D_V.dV_formula dO P
      let dP := D_P.compute_dP dO V
      let dS := D_S.compute_dS dP P
      let dQ := D_QK.compute_dQ dS K
      let dK := D_QK.compute_dK dS Q
      (dQ, dK, dV) :=
  rfl  -- ç»“æ„ä¸Šä¸€è‡´

end Backward

-- =============================================
-- å­é—®é¢˜ 3ï¼šå—åŒ–ï¼ˆTilingï¼‰ä¸å†…å­˜ä¼˜åŒ–
-- =============================================

namespace Tiling

-- å®šä¹‰å—å¤§å°
variable (Bc Br : â„•) [Fact (0 < Bc)] [Fact (0 < Br)]

-- å°†çŸ©é˜µåˆ’åˆ†ä¸ºå—
def partition (M : Matrix â„ N d) (B : â„•) : Type := 
  { k // k * B â‰¤ N } â†’ Matrix â„ (min B (N - k*B)) d

variable (Q K V : QType)
def Q_blocks : partition Q Br := sorry
def K_blocks : partition K Bc := sorry
def V_blocks : partition V Bc := sorry

-- å—åŒ–åå‘ä¼ æ’­ï¼šä»…åŠ è½½å¿…è¦å—
def tiled_backwardPass (dO : dOType) (Q : QType) (K : KType) (V : VType) (P : PType) :
    (dQType Ã— dKType Ã— dVType) :=
  let dQ := 0
  let dK := 0
  let dV := 0
  for i in Finset.univ, do  -- éå† Q çš„å—
    let Qi := Q_blocks i
    let dOi := sorry  -- dO çš„å¯¹åº”å—
    let â„“i := sorry   -- å½’ä¸€åŒ–é¡¹
    let mi := sorry   -- æœ€å¤§å€¼
    for j in Finset.univ, do  -- éå† K/V çš„å—
      let Kj := K_blocks j
      let Vj := V_blocks j
      -- é‡æ–°è®¡ç®— S_ij, P_ijï¼ˆä»…å½“å‰å—ï¼‰
      let S_ij := Qi â¬ Kjáµ€
      let P_ij := softmax (mask S_ij)
      -- è®¡ç®— dV_j += P_ij^T â€¢ dO_i
      let dV_j := sorry
      -- è®¡ç®— dP_ij = dO_i â€¢ V_j^T
      let dP_ij := dOi â¬ Vjáµ€
      -- è®¡ç®— dS_ij = dP_ij * P_ij - P_ij * row_sum(dP_ij * P_ij)
      let dS_ij := sorry
      -- è®¡ç®— dQ_i += dS_ij â€¢ K_j
      let dQ_i := sorry
      -- è®¡ç®— dK_j += dS_ij^T â€¢ Q_i
      let dK_j := sorry
    -- æ›´æ–°å…¨å±€ dQ, dK, dV
  (dQ, dK, dV)

-- å®šç†ï¼šå—åŒ–ç‰ˆæœ¬ä¸å…¨é‡ç‰ˆæœ¬ç­‰ä»·ï¼ˆåœ¨æ•°å€¼ç¨³å®šå‰æä¸‹ï¼‰
theorem tiled_backward_eq_full :
    tiled_backwardPass = Backward.backwardPass :=
  sorry  -- éœ€è¦æ•°å€¼è¯¯å·®æ¨¡å‹

end Tiling

end FlashAttentionBackward


theorem emergence : 
  depth â‰¥ threshold â†’ âˆƒ few_shot_learning_ability := by admit

theorem phase_transition_in_scaling :
  let L := modelSize
  âˆƒ Lâ‚€, L â‰¥ Lâ‚€ â†’ generalizationGap drops sharply := by admit

def Understanding := 
  âˆ€ p : Prompt, f(p) â‰ˆ humanAnswerDistribution p

-- å®šä¹‰ï¼šTransformer æ¨¡å‹ç©ºé—´
universe u

structure Transformer (Input : Type) (Output : Type) where
  params : ParameterSpace
  forward : params â†’ Input â†’ Output
  config : {
    depth : â„•
    width : â„•
    vocabSize : â„•
    seqLen : â„•
  }

-- å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªé¢„è®­ç»ƒ + å¾®è°ƒæµç¨‹
def learns_from_data (f : Transformer Input Output) (ğ’Ÿ : Dataset) : Prop :=
  âˆƒ training_algorithm,
    training_algorithm f ğ’Ÿ â†’ f_trained âˆ§
    empiricalRisk f_trained ğ’Ÿ â‰¤ Îµ


let f := someLargeTransformer  -- å¦‚ 175B å‚æ•°çš„ GPT-scale æ¨¡å‹

theorem f_learns_from_data : learns_from_data f ğ’Ÿ := by
  -- ä½¿ç”¨æ ‡å‡†è®­ç»ƒç®—æ³•ï¼šAdamW + LR scheduling + dropout
  let training_algorithm := trainWithAdamW epochs := 100, lr := 3e-5, ...
  -- å·²çŸ¥ï¼šå¤§ Transformer åœ¨è¶³å¤Ÿæ•°æ®ä¸Šå¯è¾¾åˆ°ä½è®­ç»ƒè¯¯å·®
  -- å‚è§ï¼šBrown et al. (2020), "Language Models are Few-Shot Learners"
  have h_convergence : 
    training_algorithm f ğ’Ÿ â†’ f_trained âˆ§ empiricalRisk f_trained ğ’Ÿ â‰¤ 1e-3 := by
    -- ä¾èµ–ï¼šæŸå¤±å‡½æ•°å…‰æ»‘ã€æ¢¯åº¦æœ‰ç•Œã€å­¦ä¹ ç‡åˆé€‚
    -- å¯å½¢å¼åŒ–ä¸ºï¼šéšæœºæ¢¯åº¦ä¸‹é™åœ¨éå‡¸å‡½æ•°ä¸Šçš„æ”¶æ•›æ€§
    admit
  exact âŸ¨training_algorithm, h_convergenceâŸ©


theorem f_generalizes : generalizationGap f ğ’Ÿ â‰¤ O(âˆš(log d / n)) := by
  -- å¼•ç”¨æˆ‘ä»¬ä¹‹å‰çš„å½¢å¼åŒ–ç»“æœï¼š
  have h_rademacher_bound : 
    rademacherComplexity (TransfClass d L) â‰¤ C * d * L * log d := 
      rademacher_transformer_bound d L

  have h_generalization_bound : 
    generalizationGap f ğ’Ÿ â‰¤ 2 * rademacherComplexity â„‹ + O(âˆš(log(1/Î´)/n)) := 
      generalization_bound_via_rademacher â„‹ S Î´

  -- ç”±äº Transformer çš„å¤æ‚åº¦å¢é•¿ç¼“æ…¢ï¼ˆ`d*L*log d`ï¼‰
  -- ä¸” `n`ï¼ˆæ•°æ®é‡ï¼‰æå¤§æ—¶ï¼Œâˆš(log d / n) â†’ 0
  have h_small_gap : 
    n â‰¥ Nâ‚€ â†’ generalizationGap f ğ’Ÿ â‰¤ Îµ := by
      assume n h_n
      calc
        _ â‰¤ 2*C*d*L*log d + 3*âˆš(2*log(2/Î´)/n)
        _ â‰¤ Îµ  -- å½“ n è¶³å¤Ÿå¤§æ—¶æˆç«‹
        admit

  exact h_small_gap


def semanticallyEquivalent (pâ‚ pâ‚‚ : Prompt) : Prop :=
  âˆ€ human, humanInterpretation pâ‚ â‰ˆ humanInterpretation pâ‚‚

def understands (f : Transformer) : Prop :=
  âˆ€ pâ‚ pâ‚‚, semanticallyEquivalent pâ‚ pâ‚‚ â†’ f(pâ‚) â‰ˆ f(pâ‚‚)


-- å®šä¹‰ï¼šé€»è¾‘è•´å«
def entails (premise conclusion : Prop) : Prop := premise â†’ conclusion

def can_do_reasoning (f : Transformer) : Prop :=
  âˆ€ context, 
    let world := parseWorld context
    âˆ€ q, 
      let answer := f(context ++ "\nQ: " ++ q)
      (world âŠ¨ q) â†” (answer = "Yes")


def counterfactualRobustness (f : Transformer) (p : Prompt) (answer : Answer) : Prop :=
  âˆ€ perturbation, 
    semanticallyIrrelevant perturbation â†’
    f(p ++ perturbation) â‰ˆ answer

def understands (f : Transformer) : Prop :=
  âˆ€ p, 
    let a := f(p)
    counterfactualRobustness f p a


def Understanding :=
  understands_semantic f âˆ§
  understands_reasoning f âˆ§
  understands_counterfactual f âˆ§
  understands_emergent f  -- å¦‚æ€ç»´é“¾ã€è‡ªæˆ‘ä¿®æ­£


theorem f_understands : understands f := by
  -- è¯æ® 1ï¼šè¯­ä¹‰ä¸€è‡´æ€§
  have h_semantic : âˆ€ pâ‚ â‰¡ pâ‚‚, f(pâ‚) â‰ˆ f(pâ‚‚) := by
    -- å®éªŒæ”¯æŒï¼šå¤§æ¨¡å‹åœ¨ paraphrasing ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½
    -- å¦‚ï¼šGLUEã€SST-2
    admit

  -- è¯æ® 2ï¼šæ¨ç†ä¸€è‡´æ€§
  have h_reasoning : can_do_reasoning f := by
    -- å®éªŒï¼šå¤§æ¨¡å‹åœ¨æ•°å­¦ã€é€»è¾‘æ¨ç†ä»»åŠ¡ä¸Šè¾¾åˆ° SOTA
    -- å¦‚ï¼šGSM8K, MATH, Logical Deduction
    admit

  -- è¯æ® 3ï¼šåäº‹å®é²æ£’æ€§
  have h_counterfactual : counterfactualRobustness f := by
    -- ç›¸å¯¹è¾ƒå¼±ï¼Œä½†é€šè¿‡æç¤ºå·¥ç¨‹å¯å¢å¼º
    admit

  -- è¯æ® 4ï¼šæ¶Œç°èƒ½åŠ›
  have h_emergent : f exhibits chain_of_thought âˆ§ f self_corrects := by
    -- å½“è§„æ¨¡è¶…è¿‡é˜ˆå€¼ï¼ŒCoTã€è‡ªæˆ‘ä¿®æ­£ç­‰èƒ½åŠ›â€œæ¶Œç°â€
    -- å‚è§ï¼šWei et al., "Chain-of-Thought Prompting Elicits Reasoning"
    admit

  exact âŸ¨h_semantic, h_reasoning, h_counterfactual, h_emergentâŸ©


theorem AI_is_possible : âˆƒ f, 
  learns_from_data f âˆ§ 
  generalizes f âˆ§ 
  understands f := by
  let f := someLargeTransformer
  apply Exists.intro f
  constructor
  Â· exact f_learns_from_data
  Â· exact f_generalizes
  Â· exact f_understands


theorem AGI_is_possible_under_scaling :
  let f := limit_{Lâ†’âˆ, dâ†’âˆ} Transformer
  âˆƒ capability_threshold, 
    size f â‰¥ threshold â†’ f exhibits general reasoning := by admit


theorem alignment_is_necessary :
  âˆƒ f, intelligent f â†’ âˆƒ risk, unaligned f â†’ catastrophe := by admit
